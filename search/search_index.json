{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Generate text with LLMs","text":"<p> Robust prompting &amp; (structured) text generation  Get started  Join the Community  API <pre><code>pip install outlines\n</code></pre> <p></p>"},{"location":"installation/","title":"Installation","text":"<p>You can install Outlines with <code>pip</code>:</p> <pre><code>pip install outlines\n</code></pre> <p>Outlines supports OpenAI, transformers, Mamba, llama.cpp and exllama2 but you will need to install them manually:</p> <pre><code>pip install openai\npip install transformers datasets accelerate\npip install llama-cpp-python\npip install mamba_ssm\n</code></pre> <p>If you encounter any problem using Outlines with these libraries, take a look at their installation instructions. The installation of <code>openai</code> and <code>transformers</code> should be straightforward, but other libraries have specific hardware requirements.</p>"},{"location":"installation/#installing-for-development","title":"Installing for development","text":"<p>See the contributing documentation for instructions on how to install Outlines for development.</p>"},{"location":"licence/","title":"Licence and citations","text":"<p>Outlines is licenced under the Apache 2.0 licence. To comply with the licence you need to add the following notice at the top every file that uses part of Outlines' code:</p> <pre><code>Copyright 2023- The Outlines developers\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre> <p>If you use Outlines in your work you can use the following citation:</p> <pre><code>@article{willard2023efficient,\n  title={Efficient Guided Generation for LLMs},\n  author={Willard, Brandon T and Louf, R{\\'e}mi},\n  journal={arXiv preprint arXiv:2307.09702},\n  year={2023}\n}\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>After installing Outlines, the fastest way to get to up to speed with the library is to get acquainted with its few core elements. We advise you to take a quick look at this page to see everything Outlines has to offer before diving in the documentation.</p>"},{"location":"quickstart/#core-elements","title":"Core elements","text":""},{"location":"quickstart/#models","title":"Models","text":"<p>The first step when writing a program with Outlines is to initialize a model. Weights will be loaded on the device at this step:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\n</code></pre>"},{"location":"quickstart/#text-generator","title":"Text generator","text":"<p>Once the model is initialized you can build a text generator. This generator can be called with a prompt directly, or you can use the <code>stream</code> method to generate text token by token:</p> GenerateStream <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = outlines.generate.text(model)\n\nresult = generator(\"What's 2+2?\", max_tokens=100)\n\nprint(result)\n# That's right, it's 4! But remember, a delicious and nutrient dense 4,\n# according to YEARS BUILT ON SOLID SCIENCE. This column presents additional\n# findings from the fifteen-year study that produced the 2+2=4 conclusion.\n</code></pre> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = outlines.generate.text(model)\n\nstream = generator.stream(\"What's 2+2?\", max_tokens=4)\nfor i in range(5):\n    token = next(stream)\n    print(token)\n# ['Is']\n# [' this']\n# [' even']\n# [' a']\n# [' question']\n</code></pre>"},{"location":"quickstart/#multi-label-classification","title":"Multi-label classification","text":"<p>Outlines allows you to do multi-label classification by guiding the model so it can only output either of the specified choices:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = outlines.generate.choice(model, [\"Blue\", \"Red\", \"Yellow\"])\n\ncolor = generator(\"What is the closest color to Indigo? \")\nprint(color)\n# Blue\n</code></pre>"},{"location":"quickstart/#json-structured-generation","title":"JSON-structured generation","text":"<p>Outlines can guide models so that they output valid JSON 100% of the time. You can either specify the structure using Pydantic or a string that contains a JSON Schema:</p> PydanticJSON Schema <pre><code>from enum import Enum\nfrom pydantic import BaseModel, constr, conint\n\nimport outlines\n\nclass Armor(str, Enum):\n    leather = \"leather\"\n    chainmail = \"chainmail\"\n    plate = \"plate\"\n\n\nclass Character(BaseModel):\n    name: constr(max_length=10)\n    age: conint(gt=18, lt=99)\n    armor: Armor\n    strength: conint(gt=1, lt=100)\n\nmodel = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = outlines.generate.json(model, Character)\n\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# name='Orla' age=21 armor=&lt;Armor.plate: 'plate'&gt; strength=8\n</code></pre> <pre><code>import outlines\n\nschema = \"\"\"{\n    \"$defs\": {\n        \"Armor\": {\n            \"enum\": [\"leather\", \"chainmail\", \"plate\"],\n            \"title\": \"Armor\",\n            \"type\": \"string\"\n        }\n    },\n    \"properties\": {\n        \"name\": {\"maxLength\": 10, \"title\": \"Name\", \"type\": \"string\"},\n        \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\n        \"armor\": {\"$ref\": \"#/$defs/Armor\"},\n        \"strength\": {\"title\": \"Strength\", \"type\": \"integer\"}\\\n    },\n    \"required\": [\"name\", \"age\", \"armor\", \"strength\"],\n    \"title\": \"Character\",\n    \"type\": \"object\"\n}\"\"\"\n\nmodel = outlines.models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = outlines.generate.json(model, schema)\ncharacter = generator(\n    \"Generate a new character for my awesome game: \"\n    + \"name, age (between 1 and 99), armor and strength. \"\n    )\nprint(character)\n# {'name': 'Yuki', 'age': 24, 'armor': 'plate', 'strength': 3}\n</code></pre> <p>Note</p> <p>We advise you to constrain the length of the strings fields when first testing your schema, especially with small models.</p>"},{"location":"quickstart/#grammar-structured-generation","title":"Grammar-structured generation","text":"<p>Outlines also allows to generate text that is valid to any context-free grammar (CFG) in the EBNF format. Grammars can be intimidating, but they are a very powerful tool! Indeed, they determine the syntax of every programming language, valid chess moves, molecule structure, can help with procedural graphics generation, etc.</p> <p>Here we show a simple example of a grammar that defines arithmetic operations:</p> <pre><code>from outlines import models, generate\n\narithmetic_grammar = \"\"\"\n    ?start: sum\n\n    ?sum: product\n        | sum \"+\" product   -&gt; add\n        | sum \"-\" product   -&gt; sub\n\n    ?product: atom\n        | product \"*\" atom  -&gt; mul\n        | product \"/\" atom  -&gt; div\n\n    ?atom: NUMBER           -&gt; number\n         | \"-\" atom         -&gt; neg\n         | \"(\" sum \")\"\n\n    %import common.NUMBER\n    %import common.WS_INLINE\n\n    %ignore WS_INLINE\n\"\"\"\n\nmodel = models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = generate.cfg(model, arithmetic_grammar, max_tokens=100)\n\nresult = generator(\"Question: How can you write 5*5 using addition?\\nAnswer:\")\nprint(result)\n# 5+5+5+5+5\n</code></pre> <p>EBNF grammars can be cumbersome to write. This is why Outlines provides grammar definitions in the <code>outlines.grammars.</code> module</p> <pre><code>from outlines import models, generate, grammars\n\nmodel = models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = generate.cfg(model, grammars.arithmetic, max_tokens=100)\n\nresult = generator(\"Question: How can you write 5*5 using addition?\\nAnswer:\")\nprint(result)\n# 5+5+5+5+5\n</code></pre> <p>The available grammars are listed here.</p>"},{"location":"quickstart/#regex-structured-generation","title":"Regex-structured generation","text":"<p>Slightly simpler, but no less useful, Outlines can generate text that is in the language of a regular expression. For instance to force the model to generate IP addresses:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nregex_str = r\"((25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\"\ngenerator = generate.regex(model, regex_str)\n\nresult = generator(\"What is the IP address of localhost?\\nIP: \")\nprint(result)\n# 127.0.0.100\n</code></pre>"},{"location":"quickstart/#generate-a-given-python-type","title":"Generate a given Python type","text":"<p>We provide a shortcut to regex-structured generation for simple use cases. Pass a Python type to the <code>outlines.generate.format</code> function and the LLM will output text that matches this type:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\ngenerator = generate.format(model, int)\n\nresult = generator(\"What is 2+2?\")\nprint(result)\n# 4\n</code></pre>"},{"location":"quickstart/#deploy-using-vllm-and-fastapi","title":"Deploy using vLLM and FastAPI","text":"<p>Outlines can be deployed as a LLM service using vLLM and FastAPI. The server supports asynchronous processing of incoming requests, and benefits from the performance of vLLM.</p> <p>First start the server:</p> <pre><code>python -m outlines.serve.serve --model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n</code></pre> <p>Or you can start the server with Outlines' official Docker image:</p> <pre><code>docker run -p 8000:8000 outlinesdev/outlines --model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n</code></pre> <p>This will by default start a server at <code>http://127.0.0.1:8000</code> (check what the console says, though). Without the <code>--model</code> argument set, the OPT-125M model is used.</p> <p>You can then query the model in shell by passing a prompt and a JSON Schema specification for the structure of the output:</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"Question: What is a language model? Answer:\",\n        \"schema\": {\"type\": \"string\"}\n        }'\n</code></pre> <p>Or use the requests library from another python program. You can read the vLLM documentation for more details.</p>"},{"location":"quickstart/#utilities","title":"Utilities","text":""},{"location":"quickstart/#prompt-templates","title":"Prompt templates","text":"<p>Prompting can lead to messy code. Outlines' prompt functions are python functions that contain a template for the prompt in their docstring. We use a powerful templating language to allow you to loop over lists, dictionaries, add conditionals, etc. directly from the prompt. When called, a prompt function returns the rendered template:</p> <pre><code>import outlines\n\n@outlines.prompt\ndef few_shots(instructions, examples, question):\n    \"\"\"{{ instructions }}\n\n    Examples\n    --------\n\n    {% for example in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n\n    {% endfor %}\n    Question\n    --------\n\n    Q: {{ question }}\n    A:\n    \"\"\"\n\ninstructions = \"Please answer the following question following the examples\"\nexamples = [\n    {\"question\": \"2+2=?\", \"answer\":4},\n    {\"question\": \"3+3=?\", \"answer\":6}\n]\nquestion = \"4+4 = ?\"\n\nprompt = few_shots(instructions, examples, question)\nprint(prompt)\n# Please answer the following question following the examples\n\n# Examples\n# --------\n\n# Q: 2+2=?\n# A: 4\n\n# Q: 3+3=?\n# A: 6\n\n# Question\n# --------\n\n# Q: 4+4 = ?\n# A:\n</code></pre>"},{"location":"quickstart/#outlines-functions","title":"Outlines functions","text":"<p>Once you are done experimenting with a prompt and an output structure, it is useful to be able to encapsulate all of these in a single function that can be called from other parts of the program. This is what <code>outlines.Function</code> allows you to do:</p> function.pyCall a functionCall a function stored on GitHub <pre><code>from pydantic import BaseModel\n\nimport outlines\n\n\n@outlines.prompt\ndef tell_a_joke(topic):\n    \"\"\"Tell me a joke about {{ topic }}.\"\"\"\n\nclass Joke(BaseModel):\n    setup: str\n    punchline: str\n\ngenerate_joke = outlines.Function(\n    tell_a_joke,\n    Joke,\n    \"mistralai/Mistral-7B-Instruct-v0.2\"\n)\n</code></pre> <pre><code>from .function import generate_joke\n\nresponse = generate_joke(\"baseball\")\n\n# haha\n# Joke(setup='Why was the baseball in a bad mood?', punchline='Because it got hit around a lot.')\n</code></pre> <p>You can load a function that is stored on a repository on GitHub directly from Outlines. Say <code>Someone</code> stores a function in <code>joke.py</code> at the root of the <code>TheirRepo</code> repository:</p> <p><pre><code>import outlines\n\njoke = outlines.Function.from_github(\"Someone/TheirRepo/joke\")\nresponse = joke(\"baseball\")\n</code></pre> It make it easier for the community to collaborate on the infinite number of use cases enabled by these models!</p>"},{"location":"quickstart/#going-further","title":"Going further","text":"<p>If you need more inspiration you can take a look at the cookbook. If you have any question, or requests for documentation please reach out to us on GitHub, Twitter or Discord.</p>"},{"location":"welcome/","title":"Welcome to Outlines!","text":"<p>Outlines\u3030 is a Python library that allows you to use Large Language Model in a simple and robust way (with structured generation). It is built by .txt, and is already used in production by many companies.</p>"},{"location":"welcome/#what-models-do-you-support","title":"What models do you support?","text":"<p>We support Openai, but the true power of Outlines\u3030 is unleashed with Open Source models available via the Transformers, llama.cpp, exllama2 and mamba_ssm libraries. If you want to build and maintain an integration with another library, get in touch.</p>"},{"location":"welcome/#what-are-the-main-features","title":"What are the main features?","text":"<ul> <li> <p> Make LLMs generate valid JSON</p> <p>No more invalid JSON outputs, 100% guaranteed</p> <p> Generate JSON</p> </li> <li> <p> JSON mode for vLLM</p> <p>Deploy a LLM service using Outlines' JSON structured generation and vLLM</p> <p> Deploy outlines</p> </li> <li> <p> Make LLMs follow a Regex</p> <p>Generate text that parses correctly 100% of the time</p> <p> Guide LLMs</p> </li> <li> <p> Powerful Prompt Templating</p> <p>Better manage your prompts' complexity with prompt templating</p> <p> Learn more</p> </li> </ul>"},{"location":"welcome/#why-outlines-over-alternatives","title":"Why Outlines over alternatives?","text":"<p>Outlines\u3030 is built at .txt by engineers with decades of experience in software engineering, machine learning (Bayesian Statistics and NLP), and compilers. .txt is a VC-backed company fully focused on the topic of structured generation and is committed to make the community benefit from its experience.</p> <p>We are also open source veterans and have authored/maintained many libraries over the years: the Aesara and Pythological ecosystems, Blackjax and Hy among many others. .</p> <p>Outlines does not use unnecessary abstractions that tend to get in your way. We have a laser focus on reliable text generation with LLMs, a clear roadmap to push the state of the art in this area and a commitment to clean and robust code.</p>"},{"location":"welcome/#philosophy","title":"Philosophy","text":"<p>Outlines \u3030 is a library for neural text generation. You can think of it as a more flexible replacement for the <code>generate</code> method in the transformers library.</p> <p>Outlines \u3030 helps developers structure text generation to build robust interfaces with external systems. Provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.</p> <p>Outlines \u3030 provides robust prompting primitives that separate the prompting from the execution logic and lead to simple implementations of few-shot generations, ReAct, meta-prompting, agents, etc.</p> <p>Outlines \u3030 is designed as a library that is meant to be compatible the broader ecosystem, not to replace it. We use as few abstractions as possible, and generation can be interleaved with control flow, conditionals, custom Python functions and calls to other libraries.</p> <p>Outlines \u3030 is compatible with every auto-regressive model. It only interfaces with models via the next-token logits.</p>"},{"location":"welcome/#acknowledgements","title":"Acknowledgements","text":"<p>Outlines was originally developed at @NormalComputing by @remilouf and @BrandonTWillard. It is now maintained by .txt.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/fsm/","title":"Fsm","text":""},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM","title":"<code>CFGFSM</code>","text":"<p>             Bases: <code>FSM</code></p> <p>FSM to generate text that is in the language of a context-free grammar.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>class CFGFSM(FSM):\n    \"\"\"FSM to generate text that is in the language of a context-free grammar.\"\"\"\n\n    def __init__(self, cfg_string: str, tokenizer):\n        self.cfg_string = cfg_string\n        self.tokenizer = tokenizer\n\n        self.parser = Lark(\n            cfg_string,\n            parser=\"lalr\",\n            lexer=\"contextual\",\n            propagate_positions=False,\n            maybe_placeholders=False,\n            regex=True,\n            import_paths=[grammars.GRAMMAR_PATH],\n        )\n        self.terminal_regexps = dict()\n        for terminal in self.parser.terminals:\n            if terminal.pattern is not None:\n                self.terminal_regexps[terminal.name] = terminal.pattern.to_regexp()\n        self.terminal_regexps[\"$END\"] = tokenizer.eos_token\n\n        self.generation = \"\"\n        self.reset_state = False\n        self.allow_eos = False\n        self.regex_fsm: RegexFSM\n\n        self.check_last = False\n        self.proposal_last: List[int] = []\n        self.regex_fsm_last: RegexFSM\n\n    def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n        \"\"\"Generate a list of allowed tokens for the next step.\n\n        Upon initialization, the CFG incremental parser is used to determine the\n        first regex and construct the first FSM to generate the first terminal.\n\n        This FSM is used for proposals until either:\n\n        - The FSM is exhausted, and its only remaining option is the EOS\n          token, in which case we feed the generated terminal to the\n          CFG incremental parser and allow it to propose the next regex\n          corresponding to the next set of valid terminals.\n        - The current FSM can be exhausted, but the EOS token is not the only\n          remaining option. In this case we allow proposal of current terminal extensions,\n          store the current FSM and its state, then also use the CFG parser\n          to propose a new regex corresponding to terminating the current terminal\n          and starting the next one. The model can then sample from either of these sets\n          to determine whether to extend the current terminal or terminate it and start the next one.\n\n        The CFG incremental parser is allowed to propose the EOS token from any accepting state,\n        and once it is generated, the FSM will continue to always generate the EOS token.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n\n        Returns\n        -------\n        A list that contains the tokens to mask.\n\n        \"\"\"\n        if self.is_final_state(state):\n            return [self.tokenizer.eos_token_id]\n\n        proposal = []\n        if self.generation != \"\":\n            if self.check_last:\n                proposer = self.regex_fsm_last\n            else:\n                proposer = self.regex_fsm\n            proposal += proposer.allowed_token_ids(state)\n            if self.tokenizer.eos_token_id not in proposal:\n                return proposal\n            self.check_last = False\n            proposal = [x for x in proposal if x != self.tokenizer.eos_token_id]\n            if len(proposal) &gt; 0:\n                self.check_last = True\n                self.proposal_last = proposal.copy()\n                self.regex_fsm_last = proposer\n\n        interactive = self.parser.parse_interactive(self.generation)\n        interactive.exhaust_lexer()\n\n        options = {self.terminal_regexps[x] for x in interactive.accepts()}\n        # add %ignore terminals\n        options |= {self.terminal_regexps[x] for x in self.parser.lexer_conf.ignore}\n\n        if self.terminal_regexps[\"$END\"] in options:\n            options.remove(self.terminal_regexps[\"$END\"])\n            if len(options) == 0:\n                return [self.tokenizer.eos_token_id]\n            self.allow_eos = True\n            options.add(\"\")\n            assert len(options) &gt; 1\n\n        regex_string = r\"(\" + r\"|\".join([r\"(\" + x + r\")\" for x in options]) + r\")\"\n        self.regex_fsm = RegexFSM(regex_string, self.tokenizer)\n        self.reset_state = True\n\n        proposal += self.regex_fsm.allowed_token_ids(self.first_state)\n        if self.allow_eos:\n            self.allow_eos = False\n        else:\n            proposal = [x for x in proposal if x != self.tokenizer.eos_token_id]\n            assert len(proposal) &gt; 0\n        return proposal\n\n    def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n        \"\"\"Update the state of the FSM.\n\n        Transitions the underlying regex FSM to its next state.\n        If at max tokens or EOS token, transition permanently to the final state.\n        Update stored partial generations for subsequent incremental parsing.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The new state of the FSM.\n        \"\"\"\n        if token_id == self.tokenizer.eos_token_id:\n            return self.final_state\n\n        self.generation += self.tokenizer.decode([token_id])[0]\n\n        if self.check_last:\n            if token_id in self.proposal_last:\n                return self.regex_fsm_last.next_state(state, token_id)\n            self.check_last = False\n\n        if self.reset_state:\n            self.reset_state = False\n            state = self.first_state\n\n        return self.regex_fsm.next_state(state, token_id)\n\n    def copy(self) -&gt; \"CFGFSM\":\n        \"\"\"Create a copy of the FSM.\"\"\"\n        return CFGFSM(self.cfg_string, self.tokenizer)\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.allowed_token_ids","title":"<code>allowed_token_ids(state)</code>","text":"<p>Generate a list of allowed tokens for the next step.</p> <p>Upon initialization, the CFG incremental parser is used to determine the first regex and construct the first FSM to generate the first terminal.</p> <p>This FSM is used for proposals until either:</p> <ul> <li>The FSM is exhausted, and its only remaining option is the EOS   token, in which case we feed the generated terminal to the   CFG incremental parser and allow it to propose the next regex   corresponding to the next set of valid terminals.</li> <li>The current FSM can be exhausted, but the EOS token is not the only   remaining option. In this case we allow proposal of current terminal extensions,   store the current FSM and its state, then also use the CFG parser   to propose a new regex corresponding to terminating the current terminal   and starting the next one. The model can then sample from either of these sets   to determine whether to extend the current terminal or terminate it and start the next one.</li> </ul> <p>The CFG incremental parser is allowed to propose the EOS token from any accepting state, and once it is generated, the FSM will continue to always generate the EOS token.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.allowed_token_ids--parameters","title":"Parameters","text":"<p>state     The current state of the FSM.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.allowed_token_ids--returns","title":"Returns","text":"<p>A list that contains the tokens to mask.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n    \"\"\"Generate a list of allowed tokens for the next step.\n\n    Upon initialization, the CFG incremental parser is used to determine the\n    first regex and construct the first FSM to generate the first terminal.\n\n    This FSM is used for proposals until either:\n\n    - The FSM is exhausted, and its only remaining option is the EOS\n      token, in which case we feed the generated terminal to the\n      CFG incremental parser and allow it to propose the next regex\n      corresponding to the next set of valid terminals.\n    - The current FSM can be exhausted, but the EOS token is not the only\n      remaining option. In this case we allow proposal of current terminal extensions,\n      store the current FSM and its state, then also use the CFG parser\n      to propose a new regex corresponding to terminating the current terminal\n      and starting the next one. The model can then sample from either of these sets\n      to determine whether to extend the current terminal or terminate it and start the next one.\n\n    The CFG incremental parser is allowed to propose the EOS token from any accepting state,\n    and once it is generated, the FSM will continue to always generate the EOS token.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n\n    Returns\n    -------\n    A list that contains the tokens to mask.\n\n    \"\"\"\n    if self.is_final_state(state):\n        return [self.tokenizer.eos_token_id]\n\n    proposal = []\n    if self.generation != \"\":\n        if self.check_last:\n            proposer = self.regex_fsm_last\n        else:\n            proposer = self.regex_fsm\n        proposal += proposer.allowed_token_ids(state)\n        if self.tokenizer.eos_token_id not in proposal:\n            return proposal\n        self.check_last = False\n        proposal = [x for x in proposal if x != self.tokenizer.eos_token_id]\n        if len(proposal) &gt; 0:\n            self.check_last = True\n            self.proposal_last = proposal.copy()\n            self.regex_fsm_last = proposer\n\n    interactive = self.parser.parse_interactive(self.generation)\n    interactive.exhaust_lexer()\n\n    options = {self.terminal_regexps[x] for x in interactive.accepts()}\n    # add %ignore terminals\n    options |= {self.terminal_regexps[x] for x in self.parser.lexer_conf.ignore}\n\n    if self.terminal_regexps[\"$END\"] in options:\n        options.remove(self.terminal_regexps[\"$END\"])\n        if len(options) == 0:\n            return [self.tokenizer.eos_token_id]\n        self.allow_eos = True\n        options.add(\"\")\n        assert len(options) &gt; 1\n\n    regex_string = r\"(\" + r\"|\".join([r\"(\" + x + r\")\" for x in options]) + r\")\"\n    self.regex_fsm = RegexFSM(regex_string, self.tokenizer)\n    self.reset_state = True\n\n    proposal += self.regex_fsm.allowed_token_ids(self.first_state)\n    if self.allow_eos:\n        self.allow_eos = False\n    else:\n        proposal = [x for x in proposal if x != self.tokenizer.eos_token_id]\n        assert len(proposal) &gt; 0\n    return proposal\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.copy","title":"<code>copy()</code>","text":"<p>Create a copy of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def copy(self) -&gt; \"CFGFSM\":\n    \"\"\"Create a copy of the FSM.\"\"\"\n    return CFGFSM(self.cfg_string, self.tokenizer)\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.next_state","title":"<code>next_state(state, token_id)</code>","text":"<p>Update the state of the FSM.</p> <p>Transitions the underlying regex FSM to its next state. If at max tokens or EOS token, transition permanently to the final state. Update stored partial generations for subsequent incremental parsing.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.next_state--parameters","title":"Parameters","text":"<p>state     The current state of the FSM. token_id     The id of the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.CFGFSM.next_state--returns","title":"Returns","text":"<p>The new state of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n    \"\"\"Update the state of the FSM.\n\n    Transitions the underlying regex FSM to its next state.\n    If at max tokens or EOS token, transition permanently to the final state.\n    Update stored partial generations for subsequent incremental parsing.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The new state of the FSM.\n    \"\"\"\n    if token_id == self.tokenizer.eos_token_id:\n        return self.final_state\n\n    self.generation += self.tokenizer.decode([token_id])[0]\n\n    if self.check_last:\n        if token_id in self.proposal_last:\n            return self.regex_fsm_last.next_state(state, token_id)\n        self.check_last = False\n\n    if self.reset_state:\n        self.reset_state = False\n        state = self.first_state\n\n    return self.regex_fsm.next_state(state, token_id)\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.FSM","title":"<code>FSM</code>","text":"<p>             Bases: <code>Protocol</code></p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>class FSM(Protocol):\n    first_state: FSMState = FSMState(0)\n    final_state: FSMState = FSMState(-1)\n\n    def is_final_state(self, state: FSMState) -&gt; bool:\n        \"\"\"Determine whether the current state of the FSM is a final state.\"\"\"\n        return state == self.final_state\n\n    def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n        ...\n\n    def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n        ...\n\n    def copy(self) -&gt; \"FSM\":\n        ...\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.FSM.is_final_state","title":"<code>is_final_state(state)</code>","text":"<p>Determine whether the current state of the FSM is a final state.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def is_final_state(self, state: FSMState) -&gt; bool:\n    \"\"\"Determine whether the current state of the FSM is a final state.\"\"\"\n    return state == self.final_state\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM","title":"<code>RegexFSM</code>","text":"<p>             Bases: <code>FSM</code></p> <p>FSM to generate text that is in the language of a regular expression.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>class RegexFSM(FSM):\n    \"\"\"FSM to generate text that is in the language of a regular expression.\"\"\"\n\n    def __init__(self, regex_string: str, tokenizer):\n        @cache()\n        def create_states_mapping(\n            regex_string: str, cacheable_vocabulary: Tuple[Tuple[str, int], ...]\n        ) -&gt; Tuple[dict, set]:\n            \"\"\"Create the variables related to the mapping between states and tokens\n            The parameters of the function are used for caching purpose\n            \"\"\"\n            regex_pattern = interegular.parse_pattern(regex_string)\n            regex_fsm, _ = make_deterministic_fsm(regex_pattern.to_fsm().reduce())\n            states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n                regex_fsm, tokenizer\n            )\n\n            # We make sure that it is possible to generate strings in the language\n            # of the regular expression with the tokens present in the model's\n            # vocabulary.\n            if not any(\n                regex_fsm.finals.intersection(v.values())\n                for v in states_to_token_maps.values()\n            ):\n                raise ValueError(\n                    \"The vocabulary does not allow us to build a sequence that matches the input regex\"\n                )\n\n            return states_to_token_maps, empty_token_ids\n\n        self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n            regex_string, tuple(sorted(tokenizer.vocabulary.items()))\n        )\n        self.vocabulary = list(tokenizer.vocabulary.values())\n        self.eos_token_id = tokenizer.eos_token_id\n\n    def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n        \"\"\"Generate a list of allowed tokens for the next step.\n\n        The initialization of the FSM builds an index which maps FSM states to a\n        map from authorized tokens to the state in which the FSM needs to move\n        if said token is generated. Therefore the authorized tokens at the\n        current state are the keys of the map returned by the value of the index\n        for current state.\n\n        If the current state is not contained in the end this means that we are\n        in a final state of the FSM. We only authorize EOS tokens in the final\n        state.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n\n        Returns\n        -------\n        A list that contains the tokens to mask.\n\n        \"\"\"\n        next_tokens_to_end_states = self.states_to_token_maps.get(state)\n\n        if next_tokens_to_end_states is None:\n            return [self.eos_token_id]\n        else:\n            return list(next_tokens_to_end_states.keys())\n\n    def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n        \"\"\"Update the state of the FSM.\n\n        We use the index to determine to which state the FSM should transition\n        given the token that was just generated.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The new state of the FSM.\n\n        \"\"\"\n        if token_id == self.eos_token_id:\n            return self.final_state\n\n        last_token_to_end_state = self.states_to_token_maps[state]\n        next_state = last_token_to_end_state.get(token_id)\n        if next_state is None:\n            return self.final_state\n\n        return FSMState(next_state)\n\n    @classmethod\n    def from_interegular_fsm(\n        cls, interegular_fsm: interegular.fsm.FSM, tokenizer: \"Tokenizer\"\n    ):\n        from_interegular_instance = cls.__new__(cls)\n\n        def create_states_mapping_from_interegular_fsm(\n            fsm: interegular.fsm.FSM, cacheable_vocabulary: Tuple[Tuple[str, int], ...]\n        ) -&gt; Tuple[dict, set]:\n            \"\"\"Create the variables related to the mapping between states and tokens\n            The parameters of the function are used for caching purpose\n            \"\"\"\n            regex_fsm, _ = make_deterministic_fsm(fsm.reduce())\n            states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n                regex_fsm, tokenizer\n            )\n\n            # We make sure that it is possible to generate strings in the language\n            # of the regular expression with the tokens present in the model's\n            # vocabulary.\n            if not any(\n                regex_fsm.finals.intersection(v.values())\n                for v in states_to_token_maps.values()\n            ):\n                raise ValueError(\n                    \"The vocabulary does not allow us to build a sequence that matches the input regex\"\n                )\n\n            return states_to_token_maps, empty_token_ids\n\n        (\n            from_interegular_instance.states_to_token_maps,\n            from_interegular_instance.empty_token_ids,\n        ) = create_states_mapping_from_interegular_fsm(\n            interegular_fsm, tuple(sorted(tokenizer.vocabulary.items()))\n        )\n        from_interegular_instance.vocabulary = list(tokenizer.vocabulary.values())\n        from_interegular_instance.eos_token_id = tokenizer.eos_token_id\n        return from_interegular_instance\n\n    def copy(self) -&gt; \"RegexFSM\":\n        \"\"\"Create a copy of the FSM.\"\"\"\n        return self\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.__init__","title":"<code>__init__(regex_string, tokenizer)</code>","text":"Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def __init__(self, regex_string: str, tokenizer):\n    @cache()\n    def create_states_mapping(\n        regex_string: str, cacheable_vocabulary: Tuple[Tuple[str, int], ...]\n    ) -&gt; Tuple[dict, set]:\n        \"\"\"Create the variables related to the mapping between states and tokens\n        The parameters of the function are used for caching purpose\n        \"\"\"\n        regex_pattern = interegular.parse_pattern(regex_string)\n        regex_fsm, _ = make_deterministic_fsm(regex_pattern.to_fsm().reduce())\n        states_to_token_maps, empty_token_ids = create_fsm_index_tokenizer(\n            regex_fsm, tokenizer\n        )\n\n        # We make sure that it is possible to generate strings in the language\n        # of the regular expression with the tokens present in the model's\n        # vocabulary.\n        if not any(\n            regex_fsm.finals.intersection(v.values())\n            for v in states_to_token_maps.values()\n        ):\n            raise ValueError(\n                \"The vocabulary does not allow us to build a sequence that matches the input regex\"\n            )\n\n        return states_to_token_maps, empty_token_ids\n\n    self.states_to_token_maps, self.empty_token_ids = create_states_mapping(\n        regex_string, tuple(sorted(tokenizer.vocabulary.items()))\n    )\n    self.vocabulary = list(tokenizer.vocabulary.values())\n    self.eos_token_id = tokenizer.eos_token_id\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.allowed_token_ids","title":"<code>allowed_token_ids(state)</code>","text":"<p>Generate a list of allowed tokens for the next step.</p> <p>The initialization of the FSM builds an index which maps FSM states to a map from authorized tokens to the state in which the FSM needs to move if said token is generated. Therefore the authorized tokens at the current state are the keys of the map returned by the value of the index for current state.</p> <p>If the current state is not contained in the end this means that we are in a final state of the FSM. We only authorize EOS tokens in the final state.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.allowed_token_ids--parameters","title":"Parameters","text":"<p>state     The current state of the FSM.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.allowed_token_ids--returns","title":"Returns","text":"<p>A list that contains the tokens to mask.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n    \"\"\"Generate a list of allowed tokens for the next step.\n\n    The initialization of the FSM builds an index which maps FSM states to a\n    map from authorized tokens to the state in which the FSM needs to move\n    if said token is generated. Therefore the authorized tokens at the\n    current state are the keys of the map returned by the value of the index\n    for current state.\n\n    If the current state is not contained in the end this means that we are\n    in a final state of the FSM. We only authorize EOS tokens in the final\n    state.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n\n    Returns\n    -------\n    A list that contains the tokens to mask.\n\n    \"\"\"\n    next_tokens_to_end_states = self.states_to_token_maps.get(state)\n\n    if next_tokens_to_end_states is None:\n        return [self.eos_token_id]\n    else:\n        return list(next_tokens_to_end_states.keys())\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.copy","title":"<code>copy()</code>","text":"<p>Create a copy of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def copy(self) -&gt; \"RegexFSM\":\n    \"\"\"Create a copy of the FSM.\"\"\"\n    return self\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.next_state","title":"<code>next_state(state, token_id)</code>","text":"<p>Update the state of the FSM.</p> <p>We use the index to determine to which state the FSM should transition given the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.next_state--parameters","title":"Parameters","text":"<p>state     The current state of the FSM. token_id     The id of the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.RegexFSM.next_state--returns","title":"Returns","text":"<p>The new state of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n    \"\"\"Update the state of the FSM.\n\n    We use the index to determine to which state the FSM should transition\n    given the token that was just generated.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The new state of the FSM.\n\n    \"\"\"\n    if token_id == self.eos_token_id:\n        return self.final_state\n\n    last_token_to_end_state = self.states_to_token_maps[state]\n    next_state = last_token_to_end_state.get(token_id)\n    if next_state is None:\n        return self.final_state\n\n    return FSMState(next_state)\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM","title":"<code>StopAtEosFSM</code>","text":"<p>             Bases: <code>FSM</code></p> <p>FSM to generate text until EOS has been generated.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>class StopAtEosFSM(FSM):\n    \"\"\"FSM to generate text until EOS has been generated.\"\"\"\n\n    def __init__(self, tokenizer: \"Tokenizer\"):\n        self.eos_token_id = tokenizer.eos_token_id\n        self.vocabulary = tokenizer.vocabulary.values()\n\n    def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n        \"\"\"Generate a list of allowed tokens for the next step.\n\n        When in the initial state we allow every token to be generated.\n        In the final state the only allowed token is `stop_token_id`.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n\n        Returns\n        -------\n        A list that contains the tokens to mask.\n\n        \"\"\"\n        if self.is_final_state(state):\n            return [self.eos_token_id]\n        return list(self.vocabulary)\n\n    def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n        \"\"\"Update the state of the FSM.\n\n        The FSM stays in the initial state `0` unless the specified stop token\n        has been generated or the maximum number of tokens has been reached. In\n        which case the FSM moves to the final state `-1`.\n\n        Parameters\n        ----------\n        state\n            The current state of the FSM.\n        token_id\n            The id of the token that was just generated.\n\n        Returns\n        -------\n        The new state of the FSM.\n\n        \"\"\"\n        if token_id == self.eos_token_id:\n            return self.final_state\n\n        return self.first_state\n\n    def copy(self) -&gt; \"StopAtEosFSM\":\n        \"\"\"Create a copy of the FSM.\"\"\"\n        return self\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.allowed_token_ids","title":"<code>allowed_token_ids(state)</code>","text":"<p>Generate a list of allowed tokens for the next step.</p> <p>When in the initial state we allow every token to be generated. In the final state the only allowed token is <code>stop_token_id</code>.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.allowed_token_ids--parameters","title":"Parameters","text":"<p>state     The current state of the FSM.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.allowed_token_ids--returns","title":"Returns","text":"<p>A list that contains the tokens to mask.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def allowed_token_ids(self, state: FSMState) -&gt; List[int]:\n    \"\"\"Generate a list of allowed tokens for the next step.\n\n    When in the initial state we allow every token to be generated.\n    In the final state the only allowed token is `stop_token_id`.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n\n    Returns\n    -------\n    A list that contains the tokens to mask.\n\n    \"\"\"\n    if self.is_final_state(state):\n        return [self.eos_token_id]\n    return list(self.vocabulary)\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.copy","title":"<code>copy()</code>","text":"<p>Create a copy of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def copy(self) -&gt; \"StopAtEosFSM\":\n    \"\"\"Create a copy of the FSM.\"\"\"\n    return self\n</code></pre>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.next_state","title":"<code>next_state(state, token_id)</code>","text":"<p>Update the state of the FSM.</p> <p>The FSM stays in the initial state <code>0</code> unless the specified stop token has been generated or the maximum number of tokens has been reached. In which case the FSM moves to the final state <code>-1</code>.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.next_state--parameters","title":"Parameters","text":"<p>state     The current state of the FSM. token_id     The id of the token that was just generated.</p>"},{"location":"api/fsm/#outlines.fsm.fsm.StopAtEosFSM.next_state--returns","title":"Returns","text":"<p>The new state of the FSM.</p> Source code in <code>outlines/fsm/fsm.py</code> <pre><code>def next_state(self, state: FSMState, token_id: int) -&gt; FSMState:\n    \"\"\"Update the state of the FSM.\n\n    The FSM stays in the initial state `0` unless the specified stop token\n    has been generated or the maximum number of tokens has been reached. In\n    which case the FSM moves to the final state `-1`.\n\n    Parameters\n    ----------\n    state\n        The current state of the FSM.\n    token_id\n        The id of the token that was just generated.\n\n    Returns\n    -------\n    The new state of the FSM.\n\n    \"\"\"\n    if token_id == self.eos_token_id:\n        return self.final_state\n\n    return self.first_state\n</code></pre>"},{"location":"api/json_schema/","title":"Json schema","text":""},{"location":"api/json_schema/#outlines.fsm.json_schema.build_regex_from_schema","title":"<code>build_regex_from_schema(schema, whitespace_pattern=None)</code>","text":"<p>Turn a JSON schema into a regex that matches any JSON object that follows    this schema.</p> <p>JSON Schema is a declarative language that allows to annotate JSON documents    with types and descriptions. These schemas can be generated from any Python    datastructure that has type annotation: namedtuples, dataclasses, Pydantic    models. And by ensuring that the generation respects the schema we ensure    that the output can be parsed into these objects.    This function parses the provided schema and builds a generation schedule which    mixes deterministic generation (fixed strings), and sampling with constraints.</p> <p>Parameters</p> <p>schema        A string that represents a JSON Schema.    whitespace_pattern        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)        Example: allow only a single space or newline with <code>whitespace_pattern=r\"[ ]?\"</code></p> <p>Returns</p> <p>A generation schedule. A list of strings that represent the JSON    schema's structure and regular expression that define the structure of    the fields.</p> <p>References</p> <p>.. [0] JSON Schema. https://json-schema.org/</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def build_regex_from_schema(schema: str, whitespace_pattern: Optional[str] = None):\n    \"\"\"Turn a JSON schema into a regex that matches any JSON object that follows\n    this schema.\n\n    JSON Schema is a declarative language that allows to annotate JSON documents\n    with types and descriptions. These schemas can be generated from any Python\n    datastructure that has type annotation: namedtuples, dataclasses, Pydantic\n    models. And by ensuring that the generation respects the schema we ensure\n    that the output can be parsed into these objects.\n    This function parses the provided schema and builds a generation schedule which\n    mixes deterministic generation (fixed strings), and sampling with constraints.\n\n    Parameters\n    ----------\n    schema\n        A string that represents a JSON Schema.\n    whitespace_pattern\n        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)\n        Example: allow only a single space or newline with `whitespace_pattern=r\"[\\n ]?\"`\n\n    Returns\n    -------\n    A generation schedule. A list of strings that represent the JSON\n    schema's structure and regular expression that define the structure of\n    the fields.\n\n    References\n    ----------\n    .. [0] JSON Schema. https://json-schema.org/\n\n    \"\"\"\n\n    schema = json.loads(schema)\n    Validator.check_schema(schema)\n\n    # Build reference resolver\n    schema = Resource(contents=schema, specification=DRAFT202012)\n    uri = schema.id() if schema.id() is not None else \"\"\n    registry = Registry().with_resource(uri=uri, resource=schema)\n    resolver = registry.resolver()\n\n    content = schema.contents\n    return to_regex(resolver, content, whitespace_pattern)\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.get_schema_from_signature","title":"<code>get_schema_from_signature(fn)</code>","text":"<p>Turn a function signature into a JSON schema.</p> <p>Every JSON object valid to the output JSON Schema can be passed to <code>fn</code> using the ** unpacking syntax.</p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def get_schema_from_signature(fn: Callable) -&gt; str:\n    \"\"\"Turn a function signature into a JSON schema.\n\n    Every JSON object valid to the output JSON Schema can be passed\n    to `fn` using the ** unpacking syntax.\n\n    \"\"\"\n    signature = inspect.signature(fn)\n    arguments = {}\n    for name, arg in signature.parameters.items():\n        if arg.annotation == inspect._empty:\n            raise ValueError(\"Each argument must have a type annotation\")\n        else:\n            arguments[name] = (arg.annotation, ...)\n\n    model = create_model(\"Arguments\", **arguments)\n\n    return model.model_json_schema()\n</code></pre>"},{"location":"api/json_schema/#outlines.fsm.json_schema.to_regex","title":"<code>to_regex(resolver, instance, whitespace_pattern=None)</code>","text":"<p>Translate a JSON Schema instance into a regex that validates the schema.</p> <p>Note</p> <p>Many features of JSON schema are missing:    - Handle <code>additionalProperties</code> keyword    - Handle types defined as a list    - Handle constraints on numbers    - Handle special patterns: <code>date</code>, <code>uri</code>, etc.</p> <p>This does not support recursive definitions.</p> <p>Parameters</p> <p>resolver        An object that resolves references to other instances within a schema    instance        The instance to translate    whitespace_pattern        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)        Example: allow only a single space or newline with <code>whitespace_pattern=r\"[ ]?\"</code></p> Source code in <code>outlines/fsm/json_schema.py</code> <pre><code>def to_regex(\n    resolver: Resolver, instance: dict, whitespace_pattern: Optional[str] = None\n):\n    \"\"\"Translate a JSON Schema instance into a regex that validates the schema.\n\n    Note\n    ----\n    Many features of JSON schema are missing:\n    - Handle `additionalProperties` keyword\n    - Handle types defined as a list\n    - Handle constraints on numbers\n    - Handle special patterns: `date`, `uri`, etc.\n\n    This does not support recursive definitions.\n\n    Parameters\n    ----------\n    resolver\n        An object that resolves references to other instances within a schema\n    instance\n        The instance to translate\n    whitespace_pattern\n        Pattern to use for JSON syntactic whitespace (doesn't impact string literals)\n        Example: allow only a single space or newline with `whitespace_pattern=r\"[\\n ]?\"`\n    \"\"\"\n\n    # set whitespace pattern\n    if whitespace_pattern is None:\n        whitespace_pattern = WHITESPACE\n\n    if \"properties\" in instance:\n        regex = \"\"\n        regex += r\"\\{\"\n        properties = instance[\"properties\"]\n        required_properties = instance.get(\"required\", [])\n        is_required = [item in required_properties for item in properties]\n        # If at least one property is required, we include the one in the lastest position\n        # without any comma.\n        # For each property before it (optional or required), we add with a comma after the property.\n        # For each property after it (optional), we add with a comma before the property.\n        if any(is_required):\n            last_required_pos = max([i for i, value in enumerate(is_required) if value])\n            for i, (name, value) in enumerate(properties.items()):\n                subregex = f'{whitespace_pattern}\"{name}\"{whitespace_pattern}:{whitespace_pattern}'\n                subregex += to_regex(resolver, value, whitespace_pattern)\n                if i &lt; last_required_pos:\n                    subregex = f\"{subregex}{whitespace_pattern},\"\n                elif i &gt; last_required_pos:\n                    subregex = f\"{whitespace_pattern},{subregex}\"\n                regex += subregex if is_required[i] else f\"({subregex})?\"\n        # If no property is required, we have to create a possible pattern for each property in which\n        # it's the last one necessarilly present. Then, we add the others as optional before and after\n        # following the same strategy as described above.\n        # The whole block is made optional to allow the case in which no property is returned.\n        else:\n            property_subregexes = []\n            for i, (name, value) in enumerate(properties.items()):\n                subregex = f'{whitespace_pattern}\"{name}\"{whitespace_pattern}:{whitespace_pattern}'\n                subregex += to_regex(resolver, value, whitespace_pattern)\n                property_subregexes.append(subregex)\n            possible_patterns = []\n            for i in range(len(property_subregexes)):\n                pattern = \"\"\n                for subregex in property_subregexes[:i]:\n                    pattern += f\"({subregex}{whitespace_pattern},)?\"\n                pattern += property_subregexes[i]\n                for subregex in property_subregexes[i + 1 :]:\n                    pattern += f\"({whitespace_pattern},{subregex})?\"\n                possible_patterns.append(pattern)\n            regex += f\"({'|'.join(possible_patterns)})?\"\n\n        regex += f\"{whitespace_pattern}\" + r\"\\}\"\n\n        return regex\n\n    # To validate against allOf, the given data must be valid against all of the\n    # given subschemas.\n    elif \"allOf\" in instance:\n        subregexes = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"allOf\"]\n        ]\n        subregexes_str = [f\"{subregex}\" for subregex in subregexes]\n        return rf\"({''.join(subregexes_str)})\"\n\n    # To validate against `anyOf`, the given data must be valid against\n    # any (one or more) of the given subschemas.\n    elif \"anyOf\" in instance:\n        subregexes = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"anyOf\"]\n        ]\n        return rf\"({'|'.join(subregexes)})\"\n\n    # To validate against oneOf, the given data must be valid against exactly\n    # one of the given subschemas.\n    elif \"oneOf\" in instance:\n        subregexes = [\n            to_regex(resolver, t, whitespace_pattern) for t in instance[\"oneOf\"]\n        ]\n\n        xor_patterns = []\n        # json schema validation ensured there is no overlapping schemas in oneOf\n        for subregex in subregexes:\n            other_subregexes = filter(lambda r: r != subregex, subregexes)\n            other_subregexes_str = \"|\".join([f\"{s}\" for s in other_subregexes])\n            negative_lookahead = f\"(?!.*({other_subregexes_str}))\"\n            xor_patterns.append(f\"({subregex}){negative_lookahead}\")\n\n        return rf\"({'|'.join(xor_patterns)})\"\n\n    # The enum keyword is used to restrict a value to a fixed set of values. It\n    # must be an array with at least one element, where each element is unique.\n    elif \"enum\" in instance:\n        choices = []\n        for choice in instance[\"enum\"]:\n            if type(choice) in [int, float, bool, None]:\n                choices.append(re.escape(str(choice)))\n            elif type(choice) == str:\n                choices.append(f'\"{re.escape(choice)}\"')\n\n        return f\"({'|'.join(choices)})\"\n\n    elif \"$ref\" in instance:\n        path = f\"{instance['$ref']}\"\n        instance = resolver.lookup(path).contents\n        return to_regex(resolver, instance, whitespace_pattern)\n\n    # The type keyword may either be a string or an array:\n    # - If it's a string, it is the name of one of the basic types.\n    # - If it is an array, it must be an array of strings, where each string is\n    # the name of one of the basic types, and each element is unique. In this\n    # case, the JSON snippet is valid if it matches any of the given types.\n    elif \"type\" in instance:\n        instance_type = instance[\"type\"]\n        if instance_type == \"string\":\n            if \"maxLength\" in instance or \"minLength\" in instance:\n                max_items = instance.get(\"maxLength\", \"\")\n                min_items = instance.get(\"minLength\", \"\")\n                try:\n                    if int(max_items) &lt; int(min_items):\n                        raise ValueError(\n                            \"maxLength must be greater than or equal to minLength\"\n                        )\n                except ValueError:\n                    pass\n                return f'\"{STRING_INNER}{{{min_items},{max_items}}}\"'\n            elif \"pattern\" in instance:\n                pattern = instance[\"pattern\"]\n                if pattern[0] == \"^\" and pattern[-1] == \"$\":\n                    return rf'(^\"{pattern[1:-1]}\"$)'\n                else:\n                    return rf'(\"{pattern}\")'\n            elif \"format\" in instance:\n                format = instance[\"format\"]\n                if format == \"date-time\":\n                    return format_to_regex[\"date-time\"]\n                elif format == \"uuid\":\n                    return format_to_regex[\"uuid\"]\n                elif format == \"date\":\n                    return format_to_regex[\"date\"]\n                elif format == \"time\":\n                    return format_to_regex[\"time\"]\n                else:\n                    raise NotImplementedError(\n                        f\"Format {format} is not supported by Outlines\"\n                    )\n            else:\n                return type_to_regex[\"string\"]\n\n        elif instance_type == \"number\":\n            return type_to_regex[\"number\"]\n\n        elif instance_type == \"integer\":\n            return type_to_regex[\"integer\"]\n\n        elif instance_type == \"array\":\n            num_repeats = _get_num_items_pattern(\n                instance.get(\"minItems\"), instance.get(\"maxItems\"), whitespace_pattern\n            )\n            if num_repeats is None:\n                return rf\"\\[{whitespace_pattern}\\]\"\n\n            allow_empty = \"?\" if int(instance.get(\"minItems\", 0)) == 0 else \"\"\n\n            if \"items\" in instance:\n                items_regex = to_regex(resolver, instance[\"items\"], whitespace_pattern)\n                return rf\"\\[{whitespace_pattern}(({items_regex})(,{whitespace_pattern}({items_regex})){num_repeats}){allow_empty}{whitespace_pattern}\\]\"\n            else:\n                # Here we need to make the choice to exclude generating list of objects\n                # if the specification of the object is not given, even though a JSON\n                # object that contains an object here would be valid under the specification.\n                types = [\n                    {\"type\": \"boolean\"},\n                    {\"type\": \"null\"},\n                    {\"type\": \"number\"},\n                    {\"type\": \"integer\"},\n                    {\"type\": \"string\"},\n                ]\n                regexes = [to_regex(resolver, t, whitespace_pattern) for t in types]\n                return rf\"\\[{whitespace_pattern}({'|'.join(regexes)})(,{whitespace_pattern}({'|'.join(regexes)})){num_repeats}){allow_empty}{whitespace_pattern}\\]\"\n\n        elif instance_type == \"object\":\n            # pattern for json object with values defined by instance[\"additionalProperties\"]\n            # enforces value type constraints recursively, \"minProperties\", and \"maxProperties\"\n            # doesn't enforce \"required\", \"dependencies\", \"propertyNames\" \"any/all/on Of\"\n            num_repeats = _get_num_items_pattern(\n                instance.get(\"minProperties\"),\n                instance.get(\"maxProperties\"),\n                whitespace_pattern,\n            )\n            if num_repeats is None:\n                return rf\"\\{{{whitespace_pattern}\\}}\"\n\n            allow_empty = \"?\" if int(instance.get(\"minProperties\", 0)) == 0 else \"\"\n\n            value_pattern = to_regex(\n                resolver, instance[\"additionalProperties\"], whitespace_pattern\n            )\n            key_value_pattern = (\n                f\"{STRING}{whitespace_pattern}:{whitespace_pattern}{value_pattern}\"\n            )\n            key_value_successor_pattern = (\n                f\"{whitespace_pattern},{whitespace_pattern}{key_value_pattern}\"\n            )\n            multiple_key_value_pattern = f\"({key_value_pattern}({key_value_successor_pattern}){num_repeats}){allow_empty}\"\n\n            return (\n                r\"\\{\"\n                + whitespace_pattern\n                + multiple_key_value_pattern\n                + whitespace_pattern\n                + r\"\\}\"\n            )\n\n        elif instance_type == \"boolean\":\n            return type_to_regex[\"boolean\"]\n\n        elif instance_type == \"null\":\n            return type_to_regex[\"null\"]\n\n        elif isinstance(instance_type, list):\n            # Here we need to make the choice to exclude generating an object\n            # if the specification of the object is not give, even though a JSON\n            # object that contains an object here would be valid under the specification.\n            regexes = [\n                to_regex(resolver, {\"type\": t}, whitespace_pattern)\n                for t in instance_type\n                if t != \"object\"\n            ]\n            return rf\"({'|'.join(regexes)})\"\n\n    raise NotImplementedError(\n        f\"\"\"Could not translate the instance {instance} to a\n    regular expression. Make sure it is valid to the JSON Schema specification. If\n    it is, please open an issue on the Outlines repository\"\"\"\n    )\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>Integration with OpenAI's API.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer","title":"<code>Transformer</code>","text":"<p>Represents a <code>transformers</code> model.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class Transformer:\n    \"\"\"Represents a `transformers` model.\"\"\"\n\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n    ):\n        self.device = model.device\n        self.model = model\n        self.tokenizer = tokenizer\n\n    @torch.inference_mode\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: torch.LongTensor,\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; Tuple[torch.FloatTensor, Optional[KVCacheType]]:\n        \"\"\"Compute a forward pass through the transformer model.\n\n        Parameters\n        ----------\n        input_ids\n            The input token ids.  Must be one or two dimensional.\n        attention_mask\n            The attention mask.  Must be one or two dimensional.\n        past_key_values\n            A tuple of tuples containing the cached key and value tensors for each\n            attention head.\n\n        Returns\n        -------\n        The computed logits and the new cached key and value tensors.\n\n        \"\"\"\n        assert 0 &lt; input_ids.ndim &lt; 3\n\n        if past_key_values:\n            input_ids = input_ids[..., -1].unsqueeze(-1)\n\n        output = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            output_attentions=False,\n            output_hidden_states=False,\n            past_key_values=past_key_values,\n        )\n\n        return output.logits, output.past_key_values\n\n    def __call__(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: torch.LongTensor,\n        past_key_values: Optional[Tuple] = None,\n    ) -&gt; torch.FloatTensor:\n        logits, kv_cache = self.forward(input_ids, attention_mask, past_key_values)\n        next_token_logits = logits[..., -1, :]\n\n        return next_token_logits, kv_cache\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward","title":"<code>forward(input_ids, attention_mask, past_key_values=None)</code>","text":"<p>Compute a forward pass through the transformer model.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward--parameters","title":"Parameters","text":"<p>input_ids     The input token ids.  Must be one or two dimensional. attention_mask     The attention mask.  Must be one or two dimensional. past_key_values     A tuple of tuples containing the cached key and value tensors for each     attention head.</p>"},{"location":"api/models/#outlines.models.transformers.Transformer.forward--returns","title":"Returns","text":"<p>The computed logits and the new cached key and value tensors.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>@torch.inference_mode\ndef forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: torch.LongTensor,\n    past_key_values: Optional[Tuple] = None,\n) -&gt; Tuple[torch.FloatTensor, Optional[KVCacheType]]:\n    \"\"\"Compute a forward pass through the transformer model.\n\n    Parameters\n    ----------\n    input_ids\n        The input token ids.  Must be one or two dimensional.\n    attention_mask\n        The attention mask.  Must be one or two dimensional.\n    past_key_values\n        A tuple of tuples containing the cached key and value tensors for each\n        attention head.\n\n    Returns\n    -------\n    The computed logits and the new cached key and value tensors.\n\n    \"\"\"\n    assert 0 &lt; input_ids.ndim &lt; 3\n\n    if past_key_values:\n        input_ids = input_ids[..., -1].unsqueeze(-1)\n\n    output = self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        return_dict=True,\n        output_attentions=False,\n        output_hidden_states=False,\n        past_key_values=past_key_values,\n    )\n\n    return output.logits, output.past_key_values\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.TransformerTokenizer","title":"<code>TransformerTokenizer</code>","text":"<p>             Bases: <code>Tokenizer</code></p> <p>Represents a tokenizer for models in the <code>transformers</code> library.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>class TransformerTokenizer(Tokenizer):\n    \"\"\"Represents a tokenizer for models in the `transformers` library.\"\"\"\n\n    def __init__(self, model_name: str, **kwargs):\n        from transformers import AutoTokenizer\n\n        kwargs.setdefault(\"padding_side\", \"left\")\n        self.model_name = model_name\n        # TODO: Do something to make this hashable?\n        self.kwargs = kwargs\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **kwargs)\n        self.eos_token_id = self.tokenizer.eos_token_id\n        self.eos_token = self.tokenizer.eos_token\n\n        if not self.tokenizer.pad_token_id:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n            self.pad_token_id = self.eos_token_id\n        else:\n            self.pad_token_id = self.tokenizer.pad_token_id\n            self.pad_token = self.tokenizer.pad_token\n\n        self.special_tokens = set(self.tokenizer.all_special_tokens)\n\n        self.vocabulary = self.tokenizer.get_vocab()\n        self.is_llama = isinstance(self.tokenizer, get_llama_tokenizer_types())\n\n    def encode(\n        self, prompt: Union[str, List[str]], **kwargs\n    ) -&gt; Tuple[torch.LongTensor, torch.LongTensor]:\n        kwargs[\"padding\"] = True\n        kwargs[\"return_tensors\"] = \"pt\"\n        output = self.tokenizer(prompt, **kwargs)\n        return output[\"input_ids\"], output[\"attention_mask\"]\n\n    def decode(self, token_ids: torch.LongTensor) -&gt; List[str]:\n        text = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n        return text\n\n    def convert_token_to_string(self, token: str) -&gt; str:\n        from transformers.file_utils import SPIECE_UNDERLINE\n\n        string = self.tokenizer.convert_tokens_to_string([token])\n\n        if self.is_llama:\n            # A hack to handle missing spaces to HF's Llama tokenizers\n            if token.startswith(SPIECE_UNDERLINE) or token == \"&lt;0x20&gt;\":\n                return \" \" + string\n\n        return string\n\n    def __eq__(self, other):\n        if isinstance(other, type(self)):\n            return other.model_name == self.model_name and other.kwargs == self.kwargs\n        return NotImplemented\n\n    def __hash__(self):\n        from datasets.fingerprint import Hasher\n\n        return hash(Hasher.hash(self.tokenizer))\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.get_llama_tokenizer_types","title":"<code>get_llama_tokenizer_types()</code>","text":"<p>Get all the Llama tokenizer types/classes that need work-arounds.</p> <p>When they can't be imported, a dummy class is created.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def get_llama_tokenizer_types():\n    \"\"\"Get all the Llama tokenizer types/classes that need work-arounds.\n\n    When they can't be imported, a dummy class is created.\n\n    \"\"\"\n    try:\n        from transformers.models.llama import LlamaTokenizer\n    except ImportError:\n\n        class LlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.llama import LlamaTokenizerFast\n    except ImportError:\n\n        class LlamaTokenizerFast:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizer\n    except ImportError:\n\n        class CodeLlamaTokenizer:  # type: ignore\n            pass\n\n    try:\n        from transformers.models.code_llama import CodeLlamaTokenizerFast\n    except ImportError:\n\n        class CodeLlamaTokenizerFast:  # type: ignore\n            pass\n\n    return (\n        LlamaTokenizer,\n        LlamaTokenizerFast,\n        CodeLlamaTokenizer,\n        CodeLlamaTokenizerFast,\n    )\n</code></pre>"},{"location":"api/models/#outlines.models.transformers.transformers","title":"<code>transformers(model_name, device=None, model_kwargs={}, tokenizer_kwargs={})</code>","text":"<p>Instantiate a model from the <code>transformers</code> library and its tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--parameters","title":"Parameters","text":"<p>model_name     The name of the model as listed on Hugging Face's model page. device     The device(s) on which the model should be loaded. This overrides     the <code>device_map</code> entry in <code>model_kwargs</code> when provided. model_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the model. tokenizer_kwargs     A dictionary that contains the keyword arguments to pass to the     <code>from_pretrained</code> method when loading the tokenizer.</p>"},{"location":"api/models/#outlines.models.transformers.transformers--returns","title":"Returns","text":"<p>A <code>TransformersModel</code> model instance.</p> Source code in <code>outlines/models/transformers.py</code> <pre><code>def transformers(\n    model_name: str,\n    device: Optional[str] = None,\n    model_kwargs: dict = {},\n    tokenizer_kwargs: dict = {},\n):\n    \"\"\"Instantiate a model from the `transformers` library and its tokenizer.\n\n    Parameters\n    ----------\n    model_name\n        The name of the model as listed on Hugging Face's model page.\n    device\n        The device(s) on which the model should be loaded. This overrides\n        the `device_map` entry in `model_kwargs` when provided.\n    model_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the model.\n    tokenizer_kwargs\n        A dictionary that contains the keyword arguments to pass to the\n        `from_pretrained` method when loading the tokenizer.\n\n    Returns\n    -------\n    A `TransformersModel` model instance.\n\n    \"\"\"\n    try:\n        from transformers import AutoModelForCausalLM\n    except ImportError:\n        raise ImportError(\n            \"The `transformers` library needs to be installed in order to use `transformers` models.\"\n        )\n\n    if device is not None:\n        model_kwargs[\"device_map\"] = device\n\n    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n    tokenizer = TransformerTokenizer(model_name, **tokenizer_kwargs)\n\n    return Transformer(model, tokenizer)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI","title":"<code>OpenAI</code>","text":"<p>An object that represents the OpenAI API.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>class OpenAI:\n    \"\"\"An object that represents the OpenAI API.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        api_key: Optional[str] = None,\n        max_retries: int = 6,\n        timeout: Optional[float] = None,\n        system_prompt: Optional[str] = None,\n        config: Optional[OpenAIConfig] = None,\n    ):\n        \"\"\"Create an `OpenAI` instance.\n\n        Parameters\n        ----------\n        model_name\n            Model to use, as defined in OpenAI's documentation\n        api_key\n            Secret key to use with the OpenAI API. One can also set the\n            `OPENAI_API_KEY` environment variable, or the value of\n            `openai.api_key`.\n        max_retries\n            The maximum number of retries when calls to the API fail.\n        timeout\n            Duration after which the request times out.\n        system_prompt\n            The content of the system message that precedes the user's prompt.\n        config\n            An instance of `OpenAIConfig`. Can be useful to specify some\n            parameters that cannot be set by calling this class' methods.\n\n        \"\"\"\n\n        try:\n            import openai\n        except ImportError:\n            raise ImportError(\n                \"The `openai` library needs to be installed in order to use Outlines' OpenAI integration.\"\n            )\n\n        if api_key is None:\n            if os.getenv(\"OPENAI_API_KEY\") is not None:\n                api_key = os.getenv(\"OPENAI_API_KEY\")\n            elif openai.api_key is not None:\n                api_key = openai.api_key\n            else:\n                raise ValueError(\n                    \"You must specify an API key to use the OpenAI API integration.\"\n                )\n        try:\n            client = openai.OpenAI(api_key=api_key)\n            client.models.retrieve(model_name)\n        except openai.NotFoundError:\n            raise ValueError(\n                \"Invalid model_name. Check openai models list at https://platform.openai.com/docs/models\"\n            )\n\n        if config is not None:\n            self.config = replace(config, model=model_name)  # type: ignore\n        else:\n            self.config = OpenAIConfig(model=model_name)\n\n        # This is necesssary because of an issue with the OpenAI API.\n        # Status updates: https://github.com/openai/openai-python/issues/769\n        self.create_client = functools.partial(\n            openai.AsyncOpenAI,\n            api_key=api_key,\n            max_retries=max_retries,\n            timeout=timeout,\n        )\n\n        self.system_prompt = system_prompt\n\n        # We count the total number of prompt and generated tokens as returned\n        # by the OpenAI API, summed over all the requests performed with this\n        # model instance.\n        self.prompt_tokens = 0\n        self.completion_tokens = 0\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        max_tokens: Optional[int] = None,\n        stop_at: Optional[Union[List[str], str]] = None,\n        *,\n        temperature: float = 1.0,\n        samples: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Call the OpenAI API to generate text.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        max_tokens\n            The maximum number of tokens to generate\n        temperature\n            The value of the temperature used to sample tokens\n        samples\n            The number of completions to generate for each prompt\n        stop_at\n            Up to 4 words where the API will stop the completion.\n\n        \"\"\"\n        if samples is None:\n            samples = self.config.n\n\n        config = replace(self.config, max_tokens=max_tokens, n=samples, stop=stop_at)  # type: ignore\n\n        if isinstance(stop_at, list) and len(stop_at) &gt; 4:\n            raise NotImplementedError(\n                \"The OpenAI API supports at most 4 stop sequences.\"\n            )\n\n        if \"text-\" in self.config.model:\n            raise NotImplementedError(\n                textwrap.dedent(\n                    \"Most models that support the legacy completion endpoints will be \"\n                    \"deprecated on January 2024. Use Chat models instead.\\n\"\n                    \"The list of chat models is available at https://platform.openai.com/docs/guides/text-generation.\"\n                )\n            )\n        if \"gpt-\" in self.config.model:\n            client = self.create_client()\n            response, prompt_tokens, completion_tokens = generate_chat(\n                prompt, self.system_prompt, client, config\n            )\n            self.prompt_tokens += prompt_tokens\n            self.completion_tokens += completion_tokens\n\n            return response\n\n    def stream(self, *args, **kwargs):\n        raise NotImplementedError(\n            \"Streaming is currently not supported for the OpenAI API\"\n        )\n\n    @property\n    def tokenizer(self):\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"The `tiktoken` library needs to be installed in order to choose `outlines.models.openai` with `is_in`\"\n            )\n\n        return tiktoken.encoding_for_model(self.config.model)\n\n    def generate_choice(\n        self, prompt: str, choices: List[str], max_tokens: Optional[int] = None\n    ) -&gt; str:\n        \"\"\"Call the OpenAI API to generate one of several choices.\n\n        Parameters\n        ----------\n        prompt\n            A string or list of strings that will be used to prompt the model\n        choices\n            The list of strings between which we ask the model to choose\n        max_tokens\n            The maximum number of tokens to generate\n\n        \"\"\"\n        config = replace(self.config, max_tokens=max_tokens)\n\n        greedy = False\n        decoded: List[str] = []\n        encoded_choices_left: List[List[int]] = [\n            self.tokenizer.encode(word) for word in choices\n        ]\n\n        while len(encoded_choices_left) &gt; 0:\n            max_tokens_left = max([len(tokens) for tokens in encoded_choices_left])\n            transposed_choices_left: List[Set] = [\n                {item for item in subset if item is not None}\n                for subset in zip_longest(*encoded_choices_left)\n            ]\n\n            if not greedy:\n                mask = build_optimistic_mask(transposed_choices_left)\n            else:\n                mask = {}\n                for token in transposed_choices_left[0]:  # build greedy mask\n                    mask[token] = 100\n\n            if len(mask) == 0:\n                break\n\n            config = replace(config, logit_bias=mask, max_tokens=max_tokens_left)\n\n            client = self.create_client()\n            response, prompt_tokens, completion_tokens = generate_chat(\n                prompt, self.system_prompt, client, config\n            )\n            self.prompt_tokens += prompt_tokens\n            self.completion_tokens += completion_tokens\n\n            encoded_response = self.tokenizer.encode(response)\n\n            if encoded_response in encoded_choices_left:\n                decoded.append(response)\n                break\n            else:\n                (\n                    encoded_response,\n                    encoded_choices_left,\n                ) = find_response_choices_intersection(\n                    encoded_response, encoded_choices_left\n                )\n\n                if len(encoded_response) == 0:\n                    greedy = True  # next iteration will be \"greedy\"\n                    continue\n                else:\n                    decoded.append(\"\".join(self.tokenizer.decode(encoded_response)))\n\n                    if len(encoded_choices_left) == 1:  # only one choice left\n                        choice_left = self.tokenizer.decode(encoded_choices_left[0])\n                        decoded.append(choice_left)\n                        break\n\n                    greedy = False  # after each success, stay with (or switch to) \"optimistic\" approach\n\n                prompt = prompt + \"\".join(decoded)\n\n        choice = \"\".join(decoded)\n\n        return choice\n\n    def generate_json(self):\n        \"\"\"Call the OpenAI API to generate a JSON object.\"\"\"\n        raise NotImplementedError\n\n    def __str__(self):\n        return self.__class__.__name__ + \" API\"\n\n    def __repr__(self):\n        return str(self.config)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__","title":"<code>__call__(prompt, max_tokens=None, stop_at=None, *, temperature=1.0, samples=None)</code>","text":"<p>Call the OpenAI API to generate text.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__call__--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model max_tokens     The maximum number of tokens to generate temperature     The value of the temperature used to sample tokens samples     The number of completions to generate for each prompt stop_at     Up to 4 words where the API will stop the completion.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __call__(\n    self,\n    prompt: Union[str, List[str]],\n    max_tokens: Optional[int] = None,\n    stop_at: Optional[Union[List[str], str]] = None,\n    *,\n    temperature: float = 1.0,\n    samples: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Call the OpenAI API to generate text.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    max_tokens\n        The maximum number of tokens to generate\n    temperature\n        The value of the temperature used to sample tokens\n    samples\n        The number of completions to generate for each prompt\n    stop_at\n        Up to 4 words where the API will stop the completion.\n\n    \"\"\"\n    if samples is None:\n        samples = self.config.n\n\n    config = replace(self.config, max_tokens=max_tokens, n=samples, stop=stop_at)  # type: ignore\n\n    if isinstance(stop_at, list) and len(stop_at) &gt; 4:\n        raise NotImplementedError(\n            \"The OpenAI API supports at most 4 stop sequences.\"\n        )\n\n    if \"text-\" in self.config.model:\n        raise NotImplementedError(\n            textwrap.dedent(\n                \"Most models that support the legacy completion endpoints will be \"\n                \"deprecated on January 2024. Use Chat models instead.\\n\"\n                \"The list of chat models is available at https://platform.openai.com/docs/guides/text-generation.\"\n            )\n        )\n    if \"gpt-\" in self.config.model:\n        client = self.create_client()\n        response, prompt_tokens, completion_tokens = generate_chat(\n            prompt, self.system_prompt, client, config\n        )\n        self.prompt_tokens += prompt_tokens\n        self.completion_tokens += completion_tokens\n\n        return response\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__","title":"<code>__init__(model_name, api_key=None, max_retries=6, timeout=None, system_prompt=None, config=None)</code>","text":"<p>Create an <code>OpenAI</code> instance.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.__init__--parameters","title":"Parameters","text":"<p>model_name     Model to use, as defined in OpenAI's documentation api_key     Secret key to use with the OpenAI API. One can also set the     <code>OPENAI_API_KEY</code> environment variable, or the value of     <code>openai.api_key</code>. max_retries     The maximum number of retries when calls to the API fail. timeout     Duration after which the request times out. system_prompt     The content of the system message that precedes the user's prompt. config     An instance of <code>OpenAIConfig</code>. Can be useful to specify some     parameters that cannot be set by calling this class' methods.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    api_key: Optional[str] = None,\n    max_retries: int = 6,\n    timeout: Optional[float] = None,\n    system_prompt: Optional[str] = None,\n    config: Optional[OpenAIConfig] = None,\n):\n    \"\"\"Create an `OpenAI` instance.\n\n    Parameters\n    ----------\n    model_name\n        Model to use, as defined in OpenAI's documentation\n    api_key\n        Secret key to use with the OpenAI API. One can also set the\n        `OPENAI_API_KEY` environment variable, or the value of\n        `openai.api_key`.\n    max_retries\n        The maximum number of retries when calls to the API fail.\n    timeout\n        Duration after which the request times out.\n    system_prompt\n        The content of the system message that precedes the user's prompt.\n    config\n        An instance of `OpenAIConfig`. Can be useful to specify some\n        parameters that cannot be set by calling this class' methods.\n\n    \"\"\"\n\n    try:\n        import openai\n    except ImportError:\n        raise ImportError(\n            \"The `openai` library needs to be installed in order to use Outlines' OpenAI integration.\"\n        )\n\n    if api_key is None:\n        if os.getenv(\"OPENAI_API_KEY\") is not None:\n            api_key = os.getenv(\"OPENAI_API_KEY\")\n        elif openai.api_key is not None:\n            api_key = openai.api_key\n        else:\n            raise ValueError(\n                \"You must specify an API key to use the OpenAI API integration.\"\n            )\n    try:\n        client = openai.OpenAI(api_key=api_key)\n        client.models.retrieve(model_name)\n    except openai.NotFoundError:\n        raise ValueError(\n            \"Invalid model_name. Check openai models list at https://platform.openai.com/docs/models\"\n        )\n\n    if config is not None:\n        self.config = replace(config, model=model_name)  # type: ignore\n    else:\n        self.config = OpenAIConfig(model=model_name)\n\n    # This is necesssary because of an issue with the OpenAI API.\n    # Status updates: https://github.com/openai/openai-python/issues/769\n    self.create_client = functools.partial(\n        openai.AsyncOpenAI,\n        api_key=api_key,\n        max_retries=max_retries,\n        timeout=timeout,\n    )\n\n    self.system_prompt = system_prompt\n\n    # We count the total number of prompt and generated tokens as returned\n    # by the OpenAI API, summed over all the requests performed with this\n    # model instance.\n    self.prompt_tokens = 0\n    self.completion_tokens = 0\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_choice","title":"<code>generate_choice(prompt, choices, max_tokens=None)</code>","text":"<p>Call the OpenAI API to generate one of several choices.</p>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_choice--parameters","title":"Parameters","text":"<p>prompt     A string or list of strings that will be used to prompt the model choices     The list of strings between which we ask the model to choose max_tokens     The maximum number of tokens to generate</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_choice(\n    self, prompt: str, choices: List[str], max_tokens: Optional[int] = None\n) -&gt; str:\n    \"\"\"Call the OpenAI API to generate one of several choices.\n\n    Parameters\n    ----------\n    prompt\n        A string or list of strings that will be used to prompt the model\n    choices\n        The list of strings between which we ask the model to choose\n    max_tokens\n        The maximum number of tokens to generate\n\n    \"\"\"\n    config = replace(self.config, max_tokens=max_tokens)\n\n    greedy = False\n    decoded: List[str] = []\n    encoded_choices_left: List[List[int]] = [\n        self.tokenizer.encode(word) for word in choices\n    ]\n\n    while len(encoded_choices_left) &gt; 0:\n        max_tokens_left = max([len(tokens) for tokens in encoded_choices_left])\n        transposed_choices_left: List[Set] = [\n            {item for item in subset if item is not None}\n            for subset in zip_longest(*encoded_choices_left)\n        ]\n\n        if not greedy:\n            mask = build_optimistic_mask(transposed_choices_left)\n        else:\n            mask = {}\n            for token in transposed_choices_left[0]:  # build greedy mask\n                mask[token] = 100\n\n        if len(mask) == 0:\n            break\n\n        config = replace(config, logit_bias=mask, max_tokens=max_tokens_left)\n\n        client = self.create_client()\n        response, prompt_tokens, completion_tokens = generate_chat(\n            prompt, self.system_prompt, client, config\n        )\n        self.prompt_tokens += prompt_tokens\n        self.completion_tokens += completion_tokens\n\n        encoded_response = self.tokenizer.encode(response)\n\n        if encoded_response in encoded_choices_left:\n            decoded.append(response)\n            break\n        else:\n            (\n                encoded_response,\n                encoded_choices_left,\n            ) = find_response_choices_intersection(\n                encoded_response, encoded_choices_left\n            )\n\n            if len(encoded_response) == 0:\n                greedy = True  # next iteration will be \"greedy\"\n                continue\n            else:\n                decoded.append(\"\".join(self.tokenizer.decode(encoded_response)))\n\n                if len(encoded_choices_left) == 1:  # only one choice left\n                    choice_left = self.tokenizer.decode(encoded_choices_left[0])\n                    decoded.append(choice_left)\n                    break\n\n                greedy = False  # after each success, stay with (or switch to) \"optimistic\" approach\n\n            prompt = prompt + \"\".join(decoded)\n\n    choice = \"\".join(decoded)\n\n    return choice\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAI.generate_json","title":"<code>generate_json()</code>","text":"<p>Call the OpenAI API to generate a JSON object.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def generate_json(self):\n    \"\"\"Call the OpenAI API to generate a JSON object.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig","title":"<code>OpenAIConfig</code>  <code>dataclass</code>","text":"<p>Represents the parameters of the OpenAI API.</p> <p>The information was last fetched on 2023/11/20. We document below the properties that are specific to the OpenAI API. Not all these properties are supported by Outlines.</p>"},{"location":"api/models/#outlines.models.openai.OpenAIConfig--properties","title":"Properties","text":"<p>model_name     The name of the model. Available models can be found on OpenAI's website. frequence_penalty     Number between 2.0 and -2.0. Positive values penalize new tokens based on     their existing frequency in the text, logit_bias     Modifies the likelihood of specified tokens to appear in the completion.     Number between -100 (forbid) and +100 (only allows). n     The number of completions to return for each prompt. presence_penalty     Similar to frequency penalty. response_format     Specifies the format the model must output. <code>{\"type\": \"json_object\"}</code>     enables JSON mode. seed     Two completions with the same <code>seed</code> value should return the same     completion. This is however not guaranteed. stop     Up to 4 words where the API will stop the completion. temperature     Number between 0 and 2. Higher values make the output more random, while     lower values make it more deterministic. top_p     Number between 0 and 1. Parameter for nucleus sampling. user     A unique identifier for the end-user.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@dataclass(frozen=True)\nclass OpenAIConfig:\n    \"\"\"Represents the parameters of the OpenAI API.\n\n    The information was last fetched on 2023/11/20. We document below the\n    properties that are specific to the OpenAI API. Not all these properties are\n    supported by Outlines.\n\n    Properties\n    ----------\n    model_name\n        The name of the model. Available models can be found on OpenAI's website.\n    frequence_penalty\n        Number between 2.0 and -2.0. Positive values penalize new tokens based on\n        their existing frequency in the text,\n    logit_bias\n        Modifies the likelihood of specified tokens to appear in the completion.\n        Number between -100 (forbid) and +100 (only allows).\n    n\n        The number of completions to return for each prompt.\n    presence_penalty\n        Similar to frequency penalty.\n    response_format\n        Specifies the format the model must output. `{\"type\": \"json_object\"}`\n        enables JSON mode.\n    seed\n        Two completions with the same `seed` value should return the same\n        completion. This is however not guaranteed.\n    stop\n        Up to 4 words where the API will stop the completion.\n    temperature\n        Number between 0 and 2. Higher values make the output more random, while\n        lower values make it more deterministic.\n    top_p\n        Number between 0 and 1. Parameter for nucleus sampling.\n    user\n        A unique identifier for the end-user.\n\n    \"\"\"\n\n    model: str = \"\"\n    frequency_penalty: float = 0\n    logit_bias: Dict[int, int] = field(default_factory=dict)\n    max_tokens: Optional[int] = None\n    n: int = 1\n    presence_penalty: float = 0\n    response_format: Optional[Dict[str, str]] = None\n    seed: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    temperature: Optional[float] = None\n    top_p: int = 1\n    user: str = field(default_factory=str)\n</code></pre>"},{"location":"api/models/#outlines.models.openai.build_optimistic_mask","title":"<code>build_optimistic_mask(transposed, max_mask_size=300)</code>","text":"<p>We build the largest mask possible.</p> <p>Tokens are added from left to right, so if the encoded choices are e.g. <code>[[1,2], [3,4]]</code>, <code>1</code> and <code>3</code> will be added before <code>2</code> and <code>4</code>.</p>"},{"location":"api/models/#outlines.models.openai.build_optimistic_mask--parameters","title":"Parameters","text":"<p>transposed     A list of lists that contain the nth token of each choice.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def build_optimistic_mask(\n    transposed: List[Set[int]], max_mask_size: int = 300\n) -&gt; Dict[int, int]:\n    \"\"\"We build the largest mask possible.\n\n    Tokens are added from left to right, so if the encoded choices are e.g.\n    `[[1,2], [3,4]]`, `1` and `3` will be added before `2` and `4`.\n\n    Parameters\n    ----------\n    transposed\n        A list of lists that contain the nth token of each choice.\n\n    \"\"\"\n    mask: Dict[int, int] = {}\n    for tokens in transposed:\n        for token in tokens:\n            if len(mask) == max_mask_size:\n                return mask\n            mask[token] = 100\n\n    return mask\n</code></pre>"},{"location":"api/models/#outlines.models.openai.error_handler","title":"<code>error_handler(api_call_fn)</code>","text":"<p>Handle OpenAI API errors and missing API key.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def error_handler(api_call_fn: Callable) -&gt; Callable:\n    \"\"\"Handle OpenAI API errors and missing API key.\"\"\"\n\n    def call(*args, **kwargs):\n        import openai\n\n        try:\n            return api_call_fn(*args, **kwargs)\n        except (\n            openai.APITimeoutError,\n            openai.InternalServerError,\n            openai.RateLimitError,\n        ) as e:\n            raise OSError(f\"Could not connect to the OpenAI API: {e}\")\n        except (\n            openai.AuthenticationError,\n            openai.BadRequestError,\n            openai.ConflictError,\n            openai.PermissionDeniedError,\n            openai.NotFoundError,\n            openai.UnprocessableEntityError,\n        ) as e:\n            raise e\n\n    return call\n</code></pre>"},{"location":"api/models/#outlines.models.openai.find_longest_intersection","title":"<code>find_longest_intersection(response, choice)</code>","text":"<p>Find the longest intersection between the response and the choice.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def find_longest_intersection(response: List[int], choice: List[int]) -&gt; List[int]:\n    \"\"\"Find the longest intersection between the response and the choice.\"\"\"\n    for i, (token_r, token_c) in enumerate(zip_longest(response, choice)):\n        if token_r != token_c:\n            return response[:i]\n\n    return response\n</code></pre>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection","title":"<code>find_response_choices_intersection(response, choices)</code>","text":"<p>Find the longest intersection between the response and the different choices.</p> <p>Say the response is of the form <code>[1, 2, 3, 4, 5]</code> and we have the choices <code>[[1, 2], [1, 2, 3], [6, 7, 8]</code> then the function will return <code>[1, 2, 3]</code> as the intersection, and <code>[[]]</code> as the list of choices left.</p>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection--parameters","title":"Parameters","text":"<p>response     The model's response choices     The remaining possible choices</p>"},{"location":"api/models/#outlines.models.openai.find_response_choices_intersection--returns","title":"Returns","text":"<p>A tuple that contains the longest intersection between the response and the different choices, and the choices which start with this intersection, with the intersection removed.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>def find_response_choices_intersection(\n    response: List[int], choices: List[List[int]]\n) -&gt; Tuple[List[int], List[List[int]]]:\n    \"\"\"Find the longest intersection between the response and the different\n    choices.\n\n    Say the response is of the form `[1, 2, 3, 4, 5]` and we have the choices\n    `[[1, 2], [1, 2, 3], [6, 7, 8]` then the function will return `[1, 2, 3]` as the\n    intersection, and `[[]]` as the list of choices left.\n\n    Parameters\n    ----------\n    response\n        The model's response\n    choices\n        The remaining possible choices\n\n    Returns\n    -------\n    A tuple that contains the longest intersection between the response and the\n    different choices, and the choices which start with this intersection, with the\n    intersection removed.\n\n    \"\"\"\n    max_len_prefix = 0\n    choices_left = []\n    longest_prefix = []\n    for i, choice in enumerate(choices):\n        # Find the longest intersection between the response and the choice.\n        prefix = find_longest_intersection(response, choice)\n\n        if len(prefix) &gt; max_len_prefix:\n            max_len_prefix = len(prefix)\n            choices_left = [choice[len(prefix) :]]\n            longest_prefix = prefix\n\n        elif len(prefix) == max_len_prefix:\n            choices_left.append(choice[len(prefix) :])\n\n    return longest_prefix, choices_left\n</code></pre>"},{"location":"api/models/#outlines.models.openai.generate_chat","title":"<code>generate_chat(prompt, system_prompt, client, config)</code>  <code>async</code>","text":"<p>Call OpenAI's Chat Completion API.</p>"},{"location":"api/models/#outlines.models.openai.generate_chat--parameters","title":"Parameters","text":"<p>prompt     The prompt we use to start the generation. Passed to the model     with the \"user\" role. system_prompt     The system prompt, passed to the model with the \"system\" role     before the prompt. client     The API client config     An <code>OpenAIConfig</code> instance.</p>"},{"location":"api/models/#outlines.models.openai.generate_chat--returns","title":"Returns","text":"<p>A tuple that contains the model's response(s) and usage statistics.</p> Source code in <code>outlines/models/openai.py</code> <pre><code>@functools.partial(vectorize, signature=\"(),(),(),()-&gt;(s),(),()\")\nasync def generate_chat(\n    prompt: str,\n    system_prompt: Union[str, None],\n    client: \"AsyncOpenAI\",\n    config: OpenAIConfig,\n) -&gt; Tuple[np.ndarray, int, int]:\n    \"\"\"Call OpenAI's Chat Completion API.\n\n    Parameters\n    ----------\n    prompt\n        The prompt we use to start the generation. Passed to the model\n        with the \"user\" role.\n    system_prompt\n        The system prompt, passed to the model with the \"system\" role\n        before the prompt.\n    client\n        The API client\n    config\n        An `OpenAIConfig` instance.\n\n    Returns\n    -------\n    A tuple that contains the model's response(s) and usage statistics.\n\n    \"\"\"\n\n    @cache()\n    async def call_api(prompt, system_prompt, config):\n        responses = await client.chat.completions.create(\n            messages=system_message + user_message,\n            **asdict(config),  # type: ignore\n        )\n        await client.close()\n\n        return responses.model_dump()\n\n    system_message = (\n        [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n    )\n    user_message = [{\"role\": \"user\", \"content\": prompt}]\n\n    responses = await call_api(prompt, system_prompt, config)\n\n    results = np.array(\n        [responses[\"choices\"][i][\"message\"][\"content\"] for i in range(config.n)]\n    )\n    usage = responses[\"usage\"]\n\n    return results, usage[\"prompt_tokens\"], usage[\"completion_tokens\"]\n</code></pre>"},{"location":"api/parsing/","title":"Parsing","text":""},{"location":"api/parsing/#outlines.fsm.parsing.PartialIndenter","title":"<code>PartialIndenter</code>","text":"<p>             Bases: <code>Indenter</code></p> <p>An <code>Indenter</code> that doesn't reset its state every time <code>process</code> is called.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialIndenter(Indenter):\n    \"\"\"An `Indenter` that doesn't reset its state every time `process` is called.\"\"\"\n\n    def process(self, stream):\n        return self._process(stream)\n\n    def _process(self, stream):\n        for token in stream:\n            # These were previously *after* the `yield`, but that makes the\n            # state tracking unnecessarily convoluted.\n            if token.type in self.OPEN_PAREN_types:\n                self.paren_level += 1\n            elif token.type in self.CLOSE_PAREN_types:\n                self.paren_level -= 1\n                if self.paren_level &lt; 0:\n                    raise UnexpectedToken(token, [])\n\n            if token.type == self.NL_type:\n                yield from self.handle_NL(token)\n            else:\n                yield token\n\n        # TODO: What do we want to do here?\n        # while len(self.indent_level) &gt; 1:\n        #     self.indent_level.pop()\n        #     yield Token(self.DEDENT_type, \"\")\n\n    def accepts_token_type(self, token_type):\n        if token_type in self.CLOSE_PAREN_types and self.paren_level - 1 &lt; 0:\n            return False\n\n        # TODO:\n        # if token_type == self.NL_type and self.paren_level == 0:\n        #     ...\n        #     return False\n\n        return True\n\n    def __copy__(self):\n        res = type(self)()\n        res.paren_level = self.paren_level\n        res.indent_level = copy(self.indent_level)\n        return res\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(paren_level={self.paren_level!r}, indent_level={self.indent_level!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState","title":"<code>PartialParserState</code>","text":"<p>             Bases: <code>ParserState</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParserState(ParserState):\n    __slots__ = \"use_value_stack\"\n\n    def __init__(\n        self,\n        parse_conf,\n        lexer,\n        state_stack=None,\n        value_stack=None,\n        use_value_stack=False,\n    ):\n        super().__init__(\n            parse_conf, lexer, state_stack=state_stack, value_stack=value_stack\n        )\n        self.use_value_stack = use_value_stack\n\n    def feed_token(self, token, is_end=False):\n        if token.type == \"partial\":\n            # If none of the potential terminals can transition, we need to know now\n            current_state = self.state_stack[-1]\n            current_lexer = get_contextual_lexer(self.lexer).lexers[current_state]\n\n            # We have to feed the token and determine whether or not at least\n            # one terminal is consistent with the stack; otherwise, we'll miss\n            # invalid REDUCE cases.\n            # TODO: We should track separate parses conditional on possible\n            # token/symbol types, then we can coherently reuse the following\n            # results instead of recomputing it later.\n            can_transition = False\n            for terminal_info in token.value.terminals_and_info:\n                if terminal_info.terminal_name not in current_lexer.ignore_types:\n                    test_token = Token.new_borrow_pos(\n                        terminal_info.terminal_name, \"\", token\n                    )\n\n                    stack = copy(self.state_stack)\n                    try:\n                        self.feed_token_no_stack(test_token, is_end=is_end)\n                        can_transition = True\n                        break\n                    except UnexpectedToken:\n                        continue\n                    finally:\n                        self.state_stack = stack\n                else:\n                    can_transition = True\n\n            if not can_transition:\n                expected = {\n                    s\n                    for s in self.parse_conf.states[current_state].keys()\n                    if s.isupper()\n                }\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n        elif self.use_value_stack:\n            super().feed_token(token, is_end=is_end)\n        else:\n            self.feed_token_no_stack(token, is_end=is_end)\n\n    def feed_token_no_stack(self, token, is_end=False):\n        \"\"\"\n        This is a copy of `ParserState.feed_token` with all the value stack\n        steps removed.  Since we're not exactly parsing in order to obtain a\n        CST or anything similar, we can avoid the growing expense of tracking\n        the parse tree.\n        \"\"\"\n        state_stack = self.state_stack\n        states = self.parse_conf.states\n        end_state = self.parse_conf.end_state\n\n        while True:\n            state = state_stack[-1]\n            try:\n                action, arg = states[state][token.type]\n            except KeyError:\n                expected = {s for s in states[state].keys() if s.isupper()}\n                raise UnexpectedToken(\n                    token, expected, state=self, interactive_parser=None\n                )\n\n            assert arg != end_state\n\n            if action is Shift:\n                # shift once and return\n                assert not is_end\n                state_stack.append(arg)\n                return\n            else:\n                # reduce+shift as many times as necessary\n                rule = arg\n                size = len(rule.expansion)\n                if size:\n                    del state_stack[-size:]\n\n                _action, new_state = states[state_stack[-1]][rule.origin.name]\n                assert _action is Shift\n                state_stack.append(new_state)\n\n                if is_end and state_stack[-1] == end_state:\n                    return\n\n    def __copy__(self):\n        return type(self)(\n            self.parse_conf,\n            copy(self.lexer),\n            copy(self.state_stack),\n            deepcopy(self.value_stack),\n            use_value_stack=self.use_value_stack,\n        )\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(lexer={self.lexer!r}, state_stack={self.state_stack!r})\"\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParserState.feed_token_no_stack","title":"<code>feed_token_no_stack(token, is_end=False)</code>","text":"<p>This is a copy of <code>ParserState.feed_token</code> with all the value stack steps removed.  Since we're not exactly parsing in order to obtain a CST or anything similar, we can avoid the growing expense of tracking the parse tree.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def feed_token_no_stack(self, token, is_end=False):\n    \"\"\"\n    This is a copy of `ParserState.feed_token` with all the value stack\n    steps removed.  Since we're not exactly parsing in order to obtain a\n    CST or anything similar, we can avoid the growing expense of tracking\n    the parse tree.\n    \"\"\"\n    state_stack = self.state_stack\n    states = self.parse_conf.states\n    end_state = self.parse_conf.end_state\n\n    while True:\n        state = state_stack[-1]\n        try:\n            action, arg = states[state][token.type]\n        except KeyError:\n            expected = {s for s in states[state].keys() if s.isupper()}\n            raise UnexpectedToken(\n                token, expected, state=self, interactive_parser=None\n            )\n\n        assert arg != end_state\n\n        if action is Shift:\n            # shift once and return\n            assert not is_end\n            state_stack.append(arg)\n            return\n        else:\n            # reduce+shift as many times as necessary\n            rule = arg\n            size = len(rule.expansion)\n            if size:\n                del state_stack[-size:]\n\n            _action, new_state = states[state_stack[-1]][rule.origin.name]\n            assert _action is Shift\n            state_stack.append(new_state)\n\n            if is_end and state_stack[-1] == end_state:\n                return\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialParsingFrontend","title":"<code>PartialParsingFrontend</code>","text":"<p>             Bases: <code>ParsingFrontend</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialParsingFrontend(ParsingFrontend):\n    def __init__(self, lexer_conf, parser_conf, options, parser=None):\n        assert parser_conf.parser_type == \"lalr\"\n\n        options._plugins[\"LALR_Parser\"] = PartialLALRParser\n        options._plugins[\"BasicLexer\"] = PartialBasicLexer\n        options._plugins[\"ContextualLexer\"] = PartialContextualLexer\n        options._plugins[\"LexerThread\"] = PartialLexerThread\n\n        super().__init__(lexer_conf, parser_conf, options, parser=parser)\n\n        if lexer_conf.postlex:\n            self.lexer = PartialPostLexConnector(self.lexer.lexer, lexer_conf.postlex)\n\n        self._termset_fsm_info = None\n        self._symbols_to_states: Optional[\n            Dict[str, Set[Tuple[ParseStateType, Action]]]\n        ] = None\n        self._reverse_shifts: Optional[\n            Dict[ParseStateType, Dict[str, Set[ParseStateType]]]\n        ] = None\n        # self._state_transition_map: Optional[\n        #     Dict[Tuple[ParseStateType, str], Set[ParseStateType]]\n        # ] = None\n\n    def _compute_maps(\n        self,\n    ):\n        \"\"\"Compute state transition and symbols-to-states maps.\"\"\"\n        self._reverse_shifts = {}\n        self._symbols_to_states = {}\n\n        parse_table = self.parser.parser.parse_table\n\n        for from_state, symbols_to_ops in parse_table.states.items():\n            for symbol, op in symbols_to_ops.items():\n                if op[0] == Shift:\n                    symbols_to_from_states = self._reverse_shifts.setdefault(op[1], {})\n                    symbols_to_from_states.setdefault(symbol, set()).add(from_state)\n                self._symbols_to_states.setdefault(symbol, set()).add((from_state, op))\n\n        # # TODO: This approach is very wasteful.\n        # context_lexer = get_contextual_lexer(self)\n        # self._state_transition_map = {}\n        #\n        # for from_state, transitions in parse_table.states.items():\n        #     for symbol, action in transitions.items():\n        #         # TODO: Filter non-terminals\n        #         if symbol not in context_lexer.root_lexer.terminals_by_name:\n        #             continue\n        #\n        #         if action[0] is Shift:\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(action[1])\n        #             continue\n        #\n        #         antecedent_state_seqs = parse_to_terminal(self, [(from_state,)], symbol)\n        #\n        #         for antecedent_state_seq in antecedent_state_seqs:\n        #             antecedent_state = antecedent_state_seq[-1]\n        #             self._state_transition_map.setdefault(\n        #                 (from_state, symbol), set()\n        #             ).add(antecedent_state)\n\n    def _compute_termset_fsm_info(self):\n        \"\"\"Collect and return information about terminal symbol sets and their FSMs.\n\n        Terminal symbol sets (or \"termsets\") are ordered sequences of terminal\n        symbols that are used by each parser state.  Associated with each is a\n        collection of FSMs for each terminal and a single parse state FSM that is\n        the union of each terminal's FSM.\n\n        This constructs a list of tuples containing the termset, the set of\n        parse states that use the termsets, parse state FSMs, and information\n        mapping the components of the parse state FSMs to their terminal symbol\n        FSMs.\n\n        \"\"\"\n        context_lexer = get_contextual_lexer(self)\n        termsets_to_fsms = {}\n        termsets_to_parse_states: Dict[Tuple[str, ...], Set[ParseStateType]] = {}\n        for parse_state, lexer in context_lexer.lexers.items():\n            scanner = lexer.scanner\n            key = tuple(term.name for term in scanner.terminals)\n            termsets_to_fsms[key] = (scanner.fsm, scanner.fsms_to_trans_finals)\n            termsets_to_parse_states.setdefault(key, set()).add(parse_state)\n\n        self._termset_fsm_info = [\n            (\n                termset,\n                frozenset(termsets_to_parse_states[termset]),\n                fsm,\n                fsms_to_trans_finals,\n            )\n            for termset, (fsm, fsms_to_trans_finals) in termsets_to_fsms.items()\n        ]\n\n    @property\n    def termset_fsm_info(self):\n        if self._termset_fsm_info is None:\n            self._compute_termset_fsm_info()\n        return self._termset_fsm_info\n\n    @property\n    def symbols_to_states(self):\n        if self._symbols_to_states is None:\n            self._compute_maps()\n        return self._symbols_to_states\n\n    @property\n    def reverse_shifts(self):\n        if self._reverse_shifts is None:\n            self._compute_maps()\n        return self._reverse_shifts\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner","title":"<code>PartialScanner</code>","text":"<p>             Bases: <code>Scanner</code></p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>class PartialScanner(Scanner):\n    @classmethod\n    @lru_cache\n    def construct_terminal_fsm(cls, terminal):\n        # TODO: This should really be done at the lexer/parser level so that\n        # the lifetime of these objects is tied to the parser itself.\n        regex_str = terminal.pattern.to_regexp()\n        pattern = interegular.parse_pattern(regex_str)\n        fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        return fsm, pattern.prefix_postfix\n\n    def __init__(self, terminals, g_regex_flags, re_, use_bytes, match_whole=False):\n        self.terminals = terminals\n        self.g_regex_flags = g_regex_flags\n        self.use_bytes = use_bytes\n        self.match_whole = match_whole\n        self.allowed_types = {t.name for t in self.terminals}\n        self._mres = None\n\n        fsms = []\n        for t in self.terminals:\n            fsm, prefix_postfix = self.construct_terminal_fsm(t)\n\n            # TODO FIXME: We don't support this right now.\n            assert prefix_postfix == (0, 0)\n\n            fsms.append(fsm)\n\n        self.fsm, self.fsms_to_trans_finals = fsm_union(fsms)\n\n    def get_terminals_info(\n        self, fsm_state_seq\n    ) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n        \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n        terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n        for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n            get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n        ):\n            terminal_name = self.terminals[fsm_id].name\n            info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n            terminals_and_info += (info,)\n            if in_final:\n                final_terminals_and_info += (info,)\n\n        return terminals_and_info, final_terminals_and_info\n\n    def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n        \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n        start_pos = pos\n\n        if last_fsm_state_seq:\n            assert len(last_fsm_state_seq) &gt; 1\n            start_pos += len(last_fsm_state_seq) - 1\n            start_state = last_fsm_state_seq[-1]\n        else:\n            start_state = self.fsm.initial\n\n        text_part = text[start_pos:]\n\n        state_seq = walk_fsm(\n            self.fsm,\n            text_part,\n            start_state,\n            full_match=self.match_whole,\n        )\n\n        if not state_seq:\n            return None\n\n        if last_fsm_state_seq:\n            res = last_fsm_state_seq + tuple(state_seq)\n        else:\n            res = (start_state,) + tuple(state_seq)\n\n        return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.get_terminals_info","title":"<code>get_terminals_info(fsm_state_seq)</code>","text":"<p>Get the possible terminal symbols for an FSM state sequence.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def get_terminals_info(\n    self, fsm_state_seq\n) -&gt; Tuple[Tuple[PartialTerminalInfo, ...], Tuple[PartialTerminalInfo, ...]]:\n    \"\"\"Get the possible terminal symbols for an FSM state sequence.\"\"\"\n    terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    final_terminals_and_info: Tuple[PartialTerminalInfo, ...] = ()\n    for i, (fsm_id, fsm_reads_more, in_final) in enumerate(\n        get_sub_fsms_from_seq(fsm_state_seq, self.fsms_to_trans_finals)\n    ):\n        terminal_name = self.terminals[fsm_id].name\n        info = PartialTerminalInfo(i, terminal_name, fsm_reads_more, in_final)\n        terminals_and_info += (info,)\n        if in_final:\n            final_terminals_and_info += (info,)\n\n    return terminals_and_info, final_terminals_and_info\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.PartialScanner.match","title":"<code>match(text, pos, last_fsm_state_seq=None)</code>","text":"<p>Determine an FSM match over <code>text</code> starting at <code>pos</code> and continuing <code>last_fsm_state_seq</code>.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def match(self, text, pos, last_fsm_state_seq: Optional[Tuple[int, ...]] = None):\n    \"\"\"Determine an FSM match over `text` starting at `pos` and continuing `last_fsm_state_seq`.\"\"\"\n\n    start_pos = pos\n\n    if last_fsm_state_seq:\n        assert len(last_fsm_state_seq) &gt; 1\n        start_pos += len(last_fsm_state_seq) - 1\n        start_state = last_fsm_state_seq[-1]\n    else:\n        start_state = self.fsm.initial\n\n    text_part = text[start_pos:]\n\n    state_seq = walk_fsm(\n        self.fsm,\n        text_part,\n        start_state,\n        full_match=self.match_whole,\n    )\n\n    if not state_seq:\n        return None\n\n    if last_fsm_state_seq:\n        res = last_fsm_state_seq + tuple(state_seq)\n    else:\n        res = (start_state,) + tuple(state_seq)\n\n    return res\n</code></pre>"},{"location":"api/parsing/#outlines.fsm.parsing.terminals_to_fsms","title":"<code>terminals_to_fsms(lp)</code>","text":"<p>Construct a <code>dict</code> mapping terminal symbol names to their finite state machines.</p> Source code in <code>outlines/fsm/parsing.py</code> <pre><code>def terminals_to_fsms(lp: PartialLark) -&gt; Dict[str, FSM]:\n    \"\"\"Construct a ``dict`` mapping terminal symbol names to their finite state machines.\"\"\"\n\n    symbol_names_and_fsms = {}\n    for terminal in lp.terminals:\n        pattern = interegular.parse_pattern(terminal.pattern.to_regexp())\n        # TODO: Use `pyparser.terminals[0].pattern.flags`?\n        try:\n            fsm, _ = make_deterministic_fsm(pattern.to_fsm().reduce())\n        except Unsupported:\n            fsm = None\n\n        symbol_names_and_fsms[terminal.name] = fsm\n\n    return symbol_names_and_fsms\n</code></pre>"},{"location":"api/prompts/","title":"Prompts","text":""},{"location":"api/prompts/#outlines.prompts.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>Represents a prompt function.</p> <p>We return a <code>Prompt</code> class instead of a simple function so the template defined in prompt functions can be accessed.</p> Source code in <code>outlines/prompts.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"Represents a prompt function.\n\n    We return a `Prompt` class instead of a simple function so the\n    template defined in prompt functions can be accessed.\n\n    \"\"\"\n\n    template: str\n    signature: inspect.Signature\n\n    def __post_init__(self):\n        self.parameters: List[str] = list(self.signature.parameters.keys())\n\n    def __call__(self, *args, **kwargs) -&gt; str:\n        \"\"\"Render and return the template.\n\n        Returns\n        -------\n        The rendered template as a Python ``str``.\n\n        \"\"\"\n        bound_arguments = self.signature.bind(*args, **kwargs)\n        bound_arguments.apply_defaults()\n        return render(self.template, **bound_arguments.arguments)\n\n    def __str__(self):\n        return self.template\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.Prompt.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Render and return the template.</p>"},{"location":"api/prompts/#outlines.prompts.Prompt.__call__--returns","title":"Returns","text":"<p>The rendered template as a Python <code>str</code>.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def __call__(self, *args, **kwargs) -&gt; str:\n    \"\"\"Render and return the template.\n\n    Returns\n    -------\n    The rendered template as a Python ``str``.\n\n    \"\"\"\n    bound_arguments = self.signature.bind(*args, **kwargs)\n    bound_arguments.apply_defaults()\n    return render(self.template, **bound_arguments.arguments)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_description","title":"<code>get_fn_description(fn)</code>","text":"<p>Returns the first line of a callable's docstring.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_description(fn: Callable):\n    \"\"\"Returns the first line of a callable's docstring.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `description` filter only applies to callables.\")\n\n    docstring = inspect.getdoc(fn)\n    if docstring is None:\n        description = \"\"\n    else:\n        description = docstring.split(\"\\n\")[0].strip()\n\n    return description\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_name","title":"<code>get_fn_name(fn)</code>","text":"<p>Returns the name of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_name(fn: Callable):\n    \"\"\"Returns the name of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `name` filter only applies to callables.\")\n\n    if not hasattr(fn, \"__name__\"):\n        name = type(fn).__name__\n    else:\n        name = fn.__name__\n\n    return name\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_signature","title":"<code>get_fn_signature(fn)</code>","text":"<p>Return the signature of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_signature(fn: Callable):\n    \"\"\"Return the signature of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"\\(([^)]+)\\)\"), source)\n    if re_search is None:\n        signature = \"\"\n    else:\n        signature = re_search.group(1)\n\n    return signature\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_fn_source","title":"<code>get_fn_source(fn)</code>","text":"<p>Return the source code of a callable.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def get_fn_source(fn: Callable):\n    \"\"\"Return the source code of a callable.\"\"\"\n    if not callable(fn):\n        raise TypeError(\"The `source` filter only applies to callables.\")\n\n    source = textwrap.dedent(inspect.getsource(fn))\n    re_search = re.search(re.compile(r\"(\\bdef\\b.*)\", re.DOTALL), source)\n    if re_search is not None:\n        source = re_search.group(0)\n    else:\n        raise TypeError(\"Could not read the function's source code\")\n\n    return source\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_schema_dict","title":"<code>get_schema_dict(model)</code>","text":"<p>Return a pretty-printed dictionary</p> Source code in <code>outlines/prompts.py</code> <pre><code>@get_schema.register(dict)\ndef get_schema_dict(model: Dict):\n    \"\"\"Return a pretty-printed dictionary\"\"\"\n    return json.dumps(model, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.get_schema_pydantic","title":"<code>get_schema_pydantic(model)</code>","text":"<p>Return the schema of a Pydantic model.</p> Source code in <code>outlines/prompts.py</code> <pre><code>@get_schema.register(type(BaseModel))\ndef get_schema_pydantic(model: Type[BaseModel]):\n    \"\"\"Return the schema of a Pydantic model.\"\"\"\n    if not type(model) == type(BaseModel):\n        raise TypeError(\"The `schema` filter only applies to Pydantic models.\")\n\n    if hasattr(model, \"model_json_schema\"):\n        def_key = \"$defs\"\n        raw_schema = model.model_json_schema()\n    else:  # pragma: no cover\n        def_key = \"definitions\"\n        raw_schema = model.schema()\n\n    definitions = raw_schema.get(def_key, None)\n    schema = parse_pydantic_schema(raw_schema, definitions)\n\n    return json.dumps(schema, indent=2)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.parse_pydantic_schema","title":"<code>parse_pydantic_schema(raw_schema, definitions)</code>","text":"<p>Parse the output of <code>Basemodel.[schema|model_json_schema]()</code>.</p> <p>This recursively follows the references to other schemas in case of nested models. Other schemas are stored under the \"definitions\" key in the schema of the top-level model.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def parse_pydantic_schema(raw_schema, definitions):\n    \"\"\"Parse the output of `Basemodel.[schema|model_json_schema]()`.\n\n    This recursively follows the references to other schemas in case\n    of nested models. Other schemas are stored under the \"definitions\"\n    key in the schema of the top-level model.\n\n    \"\"\"\n    simple_schema = {}\n    for name, value in raw_schema[\"properties\"].items():\n        if \"description\" in value:\n            simple_schema[name] = value[\"description\"]\n        elif \"$ref\" in value:\n            refs = value[\"$ref\"].split(\"/\")\n            simple_schema[name] = parse_pydantic_schema(\n                definitions[refs[2]], definitions\n            )\n        else:\n            simple_schema[name] = f\"&lt;{name}&gt;\"\n\n    return simple_schema\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.prompt","title":"<code>prompt(fn)</code>","text":"<p>Decorate a function that contains a prompt template.</p> <p>This allows to define prompts in the docstring of a function and simplify their manipulation by providing some degree of encapsulation. It uses the <code>render</code> function internally to render templates.</p> <p>import outlines</p> <p>@outlines.prompt def build_prompt(question): ...    \"I have a ${question}\" ... prompt = build_prompt(\"How are you?\")</p> <p>This API can also be helpful in an \"agent\" context where parts of the prompt are set when the agent is initialized and never modified later. In this situation we can partially apply the prompt function at initialization.</p> <p>import outlines import functools as ft ... @outlines.prompt ... def solve_task(name: str, objective: str, task: str): ...     '''Your name is {{name}}. ..      Your overall objective is to {{objective}}. ...     Please solve the following task: {{task}} ...     ''' ... hal = ft.partial(solve_task, \"HAL\", \"Travel to Jupiter\")</p>"},{"location":"api/prompts/#outlines.prompts.prompt--returns","title":"Returns","text":"<p>A <code>Prompt</code> callable class which will render the template when called.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def prompt(fn: Callable) -&gt; Prompt:\n    \"\"\"Decorate a function that contains a prompt template.\n\n    This allows to define prompts in the docstring of a function and simplify their\n    manipulation by providing some degree of encapsulation. It uses the `render`\n    function internally to render templates.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @outlines.prompt\n    &gt;&gt;&gt; def build_prompt(question):\n    ...    \"I have a ${question}\"\n    ...\n    &gt;&gt;&gt; prompt = build_prompt(\"How are you?\")\n\n    This API can also be helpful in an \"agent\" context where parts of the prompt\n    are set when the agent is initialized and never modified later. In this situation\n    we can partially apply the prompt function at initialization.\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; import functools as ft\n    ...\n    &gt;&gt;&gt; @outlines.prompt\n    ... def solve_task(name: str, objective: str, task: str):\n    ...     '''Your name is {{name}}.\n    ..      Your overall objective is to {{objective}}.\n    ...     Please solve the following task: {{task}}\n    ...     '''\n    ...\n    &gt;&gt;&gt; hal = ft.partial(solve_task, \"HAL\", \"Travel to Jupiter\")\n\n    Returns\n    -------\n    A `Prompt` callable class which will render the template when called.\n\n    \"\"\"\n\n    signature = inspect.signature(fn)\n\n    # The docstring contains the template that will be rendered to be used\n    # as a prompt to the language model.\n    docstring = fn.__doc__\n    if docstring is None:\n        raise TypeError(\"Could not find a template in the function's docstring.\")\n\n    template = cast(str, docstring)\n\n    return Prompt(template, signature)\n</code></pre>"},{"location":"api/prompts/#outlines.prompts.render","title":"<code>render(template, **values)</code>","text":"<p>Parse a Jinaj2 template and translate it into an Outlines graph.</p> <p>This function removes extra whitespaces and linebreaks from templates to allow users to enter prompts more naturally than if they used Python's constructs directly. See the examples for a detailed explanation.</p>"},{"location":"api/prompts/#outlines.prompts.render--examples","title":"Examples","text":"<p>Outlines follow Jinja2's syntax</p> <p>import outlines outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\") I like tomatoes and tennis</p> <p>If the first line of the template is empty, <code>render</code> removes it</p> <p>from outlines import render</p> <p>tpl = ''' ... A new string''' tpl ... '\\nA new string' render(tpl) ... 'a new string'</p> <p>Similarly, <code>render</code> ignores linebreaks introduced by placing the closing quotes underneath the text:</p> <p>tpl = ''' ... A new string ... ''' tpl ... '\\nA new string\\n' render(tpl) ... 'A new string'</p> <p>If you want to insert a linebreak at the end of the rendered template, you will need to leave an empty line at the end of the template:</p> <p>tpl = ''' ... A new string ... ... ''' tpl ... '\\nA new string\\n\\n' render(tpl) ... 'A new string\\n'</p> <p><code>render</code> removes the identation in docstrings. This is particularly important when using prompt functions</p> <p>tpl = ''' ...    a string ...    and another string''' tpl ... '\\n   a string\\n   and another string' render(tpl) ... 'a string\\nand another string'</p> <p>The indentation of the first line is assumed to be the same as the second line's</p> <p>tpl = '''a string ...     and another''' tpl ... 'a string\\n    and another' render(tpl) ... 'a string\\nand another'</p> <p>To get a different indentation for the first and the second line, we can start the prompt on the string's second line:</p> <p>tpl = ''' ... First line ...   Second line''' render(tpl) ... 'First Line\\n  Second Line'</p>"},{"location":"api/prompts/#outlines.prompts.render--parameters","title":"Parameters","text":"<p>template     A string that contains a template written with the Jinja2 syntax. **values     Map from the variables in the template to their value.</p>"},{"location":"api/prompts/#outlines.prompts.render--returns","title":"Returns","text":"<p>A string that contains the rendered template.</p> Source code in <code>outlines/prompts.py</code> <pre><code>def render(template: str, **values: Optional[Dict[str, Any]]) -&gt; str:\n    r\"\"\"Parse a Jinaj2 template and translate it into an Outlines graph.\n\n    This function removes extra whitespaces and linebreaks from templates to\n    allow users to enter prompts more naturally than if they used Python's\n    constructs directly. See the examples for a detailed explanation.\n\n    Examples\n    --------\n\n    Outlines follow Jinja2's syntax\n\n    &gt;&gt;&gt; import outlines\n    &gt;&gt;&gt; outline = outlines.render(\"I like {{food}} and {{sport}}\", food=\"tomatoes\", sport=\"tennis\")\n    I like tomatoes and tennis\n\n    If the first line of the template is empty, `render` removes it\n\n    &gt;&gt;&gt; from outlines import render\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tpl = '''\n    ... A new string'''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a new string'\n\n    Similarly, `render` ignores linebreaks introduced by placing the closing quotes\n    underneath the text:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string'\n\n    If you want to insert a linebreak at the end of the rendered template, you will\n    need to leave an empty line at the end of the template:\n\n    &gt;&gt;&gt; tpl = '''\n    ... A new string\n    ...\n    ... '''\n    &gt;&gt;&gt; tpl\n    ... '\\nA new string\\n\\n'\n    &gt;&gt;&gt; render(tpl)\n    ... 'A new string\\n'\n\n    `render` removes the identation in docstrings. This is particularly important\n    when using prompt functions\n\n    &gt;&gt;&gt; tpl = '''\n    ...    a string\n    ...    and another string'''\n    &gt;&gt;&gt; tpl\n    ... '\\n   a string\\n   and another string'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another string'\n\n    The indentation of the first line is assumed to be the same as the second line's\n\n    &gt;&gt;&gt; tpl = '''a string\n    ...     and another'''\n    &gt;&gt;&gt; tpl\n    ... 'a string\\n    and another'\n    &gt;&gt;&gt; render(tpl)\n    ... 'a string\\nand another'\n\n    To get a different indentation for the first and the second line, we can start the\n    prompt on the string's second line:\n\n    &gt;&gt;&gt; tpl = '''\n    ... First line\n    ...   Second line'''\n    &gt;&gt;&gt; render(tpl)\n    ... 'First Line\\n  Second Line'\n\n    Parameters\n    ----------\n    template\n        A string that contains a template written with the Jinja2 syntax.\n    **values\n        Map from the variables in the template to their value.\n\n    Returns\n    -------\n    A string that contains the rendered template.\n\n    \"\"\"\n    # Dedent, and remove extra linebreak\n    cleaned_template = inspect.cleandoc(template)\n\n    # Add linebreak if there were any extra linebreaks that\n    # `cleandoc` would have removed\n    ends_with_linebreak = template.replace(\" \", \"\").endswith(\"\\n\\n\")\n    if ends_with_linebreak:\n        cleaned_template += \"\\n\"\n\n    # Remove extra whitespaces, except those that immediately follow a newline symbol.\n    # This is necessary to avoid introducing whitespaces after backslash `\\` characters\n    # used to continue to the next line without linebreak.\n    cleaned_template = re.sub(r\"(?![\\r\\n])(\\b\\s+)\", \" \", cleaned_template)\n\n    env = Environment(\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True,\n        undefined=StrictUndefined,\n    )\n    env.filters[\"name\"] = get_fn_name\n    env.filters[\"description\"] = get_fn_description\n    env.filters[\"source\"] = get_fn_source\n    env.filters[\"signature\"] = get_fn_signature\n    env.filters[\"schema\"] = get_schema\n\n    jinja_template = env.from_string(cleaned_template)\n\n    return jinja_template.render(**values)\n</code></pre>"},{"location":"api/regex/","title":"Regex","text":""},{"location":"api/regex/#outlines.generate.regex.regex","title":"<code>regex(model, regex_str, sampler=multinomial())</code>","text":"<p>Generate structured text in the language of a regular expression.</p>"},{"location":"api/regex/#outlines.generate.regex.regex--parameters","title":"Parameters","text":"<p>model:     An instance of <code>Transformer</code> that represents a model from the     <code>transformers</code> library. regex_str:     The regular expression that the output must follow. sampler:     The sampling algorithm to use to generate token ids from the logits     distribution.</p>"},{"location":"api/regex/#outlines.generate.regex.regex--returns","title":"Returns","text":"<p>A <code>SequenceGenerator</code> instance that generates text constrained by the regular expression.</p> Source code in <code>outlines/generate/regex.py</code> <pre><code>@singledispatch\ndef regex(model, regex_str: str, sampler: Sampler = multinomial()):\n    \"\"\"Generate structured text in the language of a regular expression.\n\n    Parameters\n    ----------\n    model:\n        An instance of `Transformer` that represents a model from the\n        `transformers` library.\n    regex_str:\n        The regular expression that the output must follow.\n    sampler:\n        The sampling algorithm to use to generate token ids from the logits\n        distribution.\n\n    Returns\n    -------\n    A `SequenceGenerator` instance that generates text constrained by the\n    regular expression.\n\n    \"\"\"\n    fsm = RegexFSM(regex_str, model.tokenizer)\n\n    device = model.device\n    generator = SequenceGenerator(fsm, model, sampler, device)\n\n    return generator\n</code></pre>"},{"location":"api/samplers/","title":"Samplers","text":""},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler","title":"<code>BeamSearchSampler</code>","text":"<p>Beam Search sampling algorithm.</p>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler--attributes","title":"Attributes","text":"<p>samples     The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class BeamSearchSampler:\n    \"\"\"Beam Search sampling algorithm.\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(self, beams: int = 1):\n        self.samples = beams\n\n    def __call__(\n        self,\n        next_token_logits: torch.DoubleTensor,\n        sequence_weights: torch.DoubleTensor,\n        _,\n    ) -&gt; Tuple[torch.DoubleTensor, torch.DoubleTensor, torch.DoubleTensor]:\n        \"\"\"Call the beam search sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n        weights = logprobs + sequence_weights.unsqueeze(1).expand_as(next_token_logits)\n\n        # Flatten scores to (n_batch, n_samples * vocab_size)\n        # and find the top-k weights for each batch.\n        batch_size = next_token_logits.shape[0] // self.samples\n        vocab_size = next_token_logits.shape[-1]\n        weights = weights.view(batch_size, self.samples * vocab_size)\n\n        # If the weights are all equal to 0 we are at the beginning of the search\n        # and thus only need to sample from one set of token logits for each\n        # batch.\n        if torch.all(sequence_weights == 0):\n            weights = weights[:, :vocab_size]\n\n        weights, indices = torch.topk(\n            weights, self.samples, dim=1, largest=True, sorted=True\n        )\n\n        ancestors = torch.div(indices, vocab_size, rounding_mode=\"floor\")\n        next_token_ids = indices % vocab_size\n\n        # Re-shape the weights, next_token_ids and ancestors to (n_batch * n_samples, 1)\n        first_batch_idx = torch.arange(\n            0, batch_size * self.samples, self.samples, device=next_token_logits.device\n        ).unsqueeze(1)\n        ancestors = ancestors + first_batch_idx\n\n        ancestors = ancestors.view(self.samples * batch_size)\n        weights = weights.view(self.samples * batch_size)\n        next_token_ids = next_token_ids.view(self.samples * batch_size, 1)\n\n        return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, _)</code>","text":"<p>Call the beam search sampler.</p>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__--parameters","title":"Parameters","text":"<p>next_token_logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. sequence_weights     A tensor of shape <code>(n_seqs,)</code> that represents the cumulative     weight of each sequence. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.samplers.BeamSearchSampler.__call__--returns","title":"Returns","text":"<p>A tuple with an array that contains the ids of the sampled tokens of shape <code>(n_seqs, 1)</code>, an array that contains the ancestors of each sampled id of shape <code>(n_seqs,)</code> and an array that contains the updated cumulative weights of each sequence of shape <code>(n_seqs,)</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: torch.DoubleTensor,\n    sequence_weights: torch.DoubleTensor,\n    _,\n) -&gt; Tuple[torch.DoubleTensor, torch.DoubleTensor, torch.DoubleTensor]:\n    \"\"\"Call the beam search sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n    weights = logprobs + sequence_weights.unsqueeze(1).expand_as(next_token_logits)\n\n    # Flatten scores to (n_batch, n_samples * vocab_size)\n    # and find the top-k weights for each batch.\n    batch_size = next_token_logits.shape[0] // self.samples\n    vocab_size = next_token_logits.shape[-1]\n    weights = weights.view(batch_size, self.samples * vocab_size)\n\n    # If the weights are all equal to 0 we are at the beginning of the search\n    # and thus only need to sample from one set of token logits for each\n    # batch.\n    if torch.all(sequence_weights == 0):\n        weights = weights[:, :vocab_size]\n\n    weights, indices = torch.topk(\n        weights, self.samples, dim=1, largest=True, sorted=True\n    )\n\n    ancestors = torch.div(indices, vocab_size, rounding_mode=\"floor\")\n    next_token_ids = indices % vocab_size\n\n    # Re-shape the weights, next_token_ids and ancestors to (n_batch * n_samples, 1)\n    first_batch_idx = torch.arange(\n        0, batch_size * self.samples, self.samples, device=next_token_logits.device\n    ).unsqueeze(1)\n    ancestors = ancestors + first_batch_idx\n\n    ancestors = ancestors.view(self.samples * batch_size)\n    weights = weights.view(self.samples * batch_size)\n    next_token_ids = next_token_ids.view(self.samples * batch_size, 1)\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.GreedySampler","title":"<code>GreedySampler</code>","text":"<p>Greedy Sampling algorithm.</p> <p>Greedy sampling consists in choosing the token with the largest likelihood at every step.</p> <p>We don't allow more than one sample. We could attribute this a meaning, for instance the k-th sample represents the k-th most likely token. In which case it would be equivalent to beam search without the sequence weights.</p>"},{"location":"api/samplers/#outlines.samplers.GreedySampler--attributes","title":"Attributes","text":"<p>samples     The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class GreedySampler:\n    \"\"\"Greedy Sampling algorithm.\n\n    Greedy sampling consists in choosing the token with the largest\n    likelihood at every step.\n\n    We don't allow more than one sample. We could attribute this a meaning, for\n    instance the k-th sample represents the k-th most likely token. In which\n    case it would be equivalent to beam search without the sequence weights.\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(self):\n        self.samples = 1\n\n    def __call__(\n        self,\n        next_token_logits: torch.DoubleTensor,\n        sequence_weights: torch.DoubleTensor,\n        _,\n    ) -&gt; torch.DoubleTensor:\n        \"\"\"Call the greedy sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n        next_token_ids = torch.argmax(logprobs, dim=-1, keepdim=True)\n\n        ancestors = torch.arange(\n            next_token_logits.shape[0], device=next_token_logits.device\n        )\n        weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n        return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, _)</code>","text":"<p>Call the greedy sampler.</p>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__--parameters","title":"Parameters","text":"<p>next_token_logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. sequence_weights     A tensor of shape <code>(n_seqs,)</code> that represents the cumulative     weight of each sequence. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.samplers.GreedySampler.__call__--returns","title":"Returns","text":"<p>A tuple with an array that contains the ids of the sampled tokens of shape <code>(n_seqs, 1)</code>, an array that contains the ancestors of each sampled id of shape <code>(n_seqs,)</code> and an array that contains the updated cumulative weights of each sequence of shape <code>(n_seqs,)</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: torch.DoubleTensor,\n    sequence_weights: torch.DoubleTensor,\n    _,\n) -&gt; torch.DoubleTensor:\n    \"\"\"Call the greedy sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    logprobs = torch.nn.functional.log_softmax(next_token_logits, dim=-1)\n    next_token_ids = torch.argmax(logprobs, dim=-1, keepdim=True)\n\n    ancestors = torch.arange(\n        next_token_logits.shape[0], device=next_token_logits.device\n    )\n    weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler","title":"<code>MultinomialSampler</code>","text":"<p>Multinomial sampling algorithm.</p> <p>Multinomial sampling consists in randomly sampling the next token assuming its distribution is a Categorical distribution parametrized by the next-token logits.</p>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler--attributes","title":"Attributes","text":"<p>samples     The number of samples taken for each input sequence.</p> Source code in <code>outlines/samplers.py</code> <pre><code>class MultinomialSampler:\n    \"\"\"Multinomial sampling algorithm.\n\n    Multinomial sampling consists in randomly sampling the next token assuming\n    its distribution is a Categorical distribution parametrized by the\n    next-token logits.\n\n\n    Attributes\n    ----------\n    samples\n        The number of samples taken for each input sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        samples: int = 1,\n        *,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        temperature: Optional[float] = None,\n    ):\n        self.samples = samples\n\n        self.logits_processors = []\n        if top_k is not None:\n            self.logits_processors.append(keep_top_k_logits(top_k))\n        elif top_p is not None:\n            self.logits_processors.append(keep_top_p_logits(top_p))\n\n        if temperature is not None:\n            self.logits_processors.append(rescale_logits(temperature))\n\n    def __call__(\n        self,\n        next_token_logits: torch.DoubleTensor,\n        sequence_weights: torch.DoubleTensor,\n        rng: torch.Generator,\n    ) -&gt; Tuple[torch.DoubleTensor, torch.DoubleTensor, torch.DoubleTensor]:\n        \"\"\"Call the multinomial sampler.\n\n        Parameters\n        ----------\n        next_token_logits\n            A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n            probability distribution of the next token over the vocabulary.\n        sequence_weights\n            A tensor of shape ``(n_seqs,)`` that represents the cumulative\n            weight of each sequence.\n        rng\n            A random number generator.\n\n        Returns\n        -------\n        A tuple with an array that contains the ids of the sampled tokens of\n        shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n        sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n        cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n        \"\"\"\n        altered_next_token_logits = next_token_logits\n        for logit_processor in self.logits_processors:\n            altered_next_token_logits = logit_processor(next_token_logits)\n\n        probs = torch.nn.functional.softmax(altered_next_token_logits, dim=-1)\n        next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n\n        logprobs = torch.nn.functional.log_softmax(altered_next_token_logits, dim=-1)\n        ancestors = torch.arange(\n            altered_next_token_logits.shape[0], device=next_token_logits.device\n        )\n        weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n        return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__","title":"<code>__call__(next_token_logits, sequence_weights, rng)</code>","text":"<p>Call the multinomial sampler.</p>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__--parameters","title":"Parameters","text":"<p>next_token_logits     A tensor of shape <code>(n_seqs, vocab_size,)</code> that represents the     probability distribution of the next token over the vocabulary. sequence_weights     A tensor of shape <code>(n_seqs,)</code> that represents the cumulative     weight of each sequence. rng     A random number generator.</p>"},{"location":"api/samplers/#outlines.samplers.MultinomialSampler.__call__--returns","title":"Returns","text":"<p>A tuple with an array that contains the ids of the sampled tokens of shape <code>(n_seqs, 1)</code>, an array that contains the ancestors of each sampled id of shape <code>(n_seqs,)</code> and an array that contains the updated cumulative weights of each sequence of shape <code>(n_seqs,)</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def __call__(\n    self,\n    next_token_logits: torch.DoubleTensor,\n    sequence_weights: torch.DoubleTensor,\n    rng: torch.Generator,\n) -&gt; Tuple[torch.DoubleTensor, torch.DoubleTensor, torch.DoubleTensor]:\n    \"\"\"Call the multinomial sampler.\n\n    Parameters\n    ----------\n    next_token_logits\n        A tensor of shape ``(n_seqs, vocab_size,)`` that represents the\n        probability distribution of the next token over the vocabulary.\n    sequence_weights\n        A tensor of shape ``(n_seqs,)`` that represents the cumulative\n        weight of each sequence.\n    rng\n        A random number generator.\n\n    Returns\n    -------\n    A tuple with an array that contains the ids of the sampled tokens of\n    shape ``(n_seqs, 1)``, an array that contains the ancestors of each\n    sampled id of shape ``(n_seqs,)`` and an array that contains the updated\n    cumulative weights of each sequence of shape ``(n_seqs,)``.\n\n    \"\"\"\n    altered_next_token_logits = next_token_logits\n    for logit_processor in self.logits_processors:\n        altered_next_token_logits = logit_processor(next_token_logits)\n\n    probs = torch.nn.functional.softmax(altered_next_token_logits, dim=-1)\n    next_token_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n\n    logprobs = torch.nn.functional.log_softmax(altered_next_token_logits, dim=-1)\n    ancestors = torch.arange(\n        altered_next_token_logits.shape[0], device=next_token_logits.device\n    )\n    weights = sequence_weights + torch.gather(logprobs, 1, next_token_ids).squeeze()\n\n    return next_token_ids, ancestors, weights\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.keep_top_k_logits","title":"<code>keep_top_k_logits(k)</code>","text":"<p>Build a function that masks logits values smaller than the top <code>k</code> ones.</p>"},{"location":"api/samplers/#outlines.samplers.keep_top_k_logits--parameters","title":"Parameters","text":"<p>k     The ranking below which logit values are replaced by <code>-math.inf</code>.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def keep_top_k_logits(k: int) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n    \"\"\"Build a function that masks logits values smaller than the top `k` ones.\n\n    Parameters\n    ----------\n    k\n        The ranking below which logit values are replaced by `-math.inf`.\n\n    \"\"\"\n    if not isinstance(k, int) or k &lt; 1:\n        raise ValueError(f\"`k` must be a strictly positive integers, got {k} instead.\")\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        num_to_keep = min(k, logits.size(-1))\n        mask_idx = logits &lt; torch.topk(logits, num_to_keep)[0][..., -1, None]\n        return logits.masked_fill(mask_idx, -math.inf)\n\n    return logits_processor\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.keep_top_p_logits","title":"<code>keep_top_p_logits(p)</code>","text":"<p>Build a function that masks the lowest probability tokens whose cumulative probability is below a certain threshold.</p>"},{"location":"api/samplers/#outlines.samplers.keep_top_p_logits--parameters","title":"Parameters","text":"<p>p     The value of the threshold. We keep the highest probability tokens whose     cumulative distribution is greater than or equal to <code>p</code> and mask the     others. Its value must be between 0 (excluded) and 1 (included).</p> Source code in <code>outlines/samplers.py</code> <pre><code>def keep_top_p_logits(p: float) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n    \"\"\"Build a function that masks the lowest probability tokens whose\n    cumulative probability is below a certain threshold.\n\n    Parameters\n    ----------\n    p\n        The value of the threshold. We keep the highest probability tokens whose\n        cumulative distribution is greater than or equal to `p` and mask the\n        others. Its value must be between 0 (excluded) and 1 (included).\n\n    \"\"\"\n    if p &lt;= 0.0 or p &gt; 1.0:\n        raise ValueError(\n            f\"`p` must be a floating point number between 0 (excluded) and 1 (included), got {p} instead.\"\n        )\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        sorted_logits, sorted_idx = torch.sort(logits, descending=False)\n        cumulative_probabilties = torch.nn.functional.softmax(\n            sorted_logits, dim=-1\n        ).cumsum(dim=-1)\n\n        sorted_masked_idx = cumulative_probabilties &lt;= (1 - p)\n        mask_idx = torch.scatter(sorted_masked_idx, 1, sorted_idx, sorted_masked_idx)\n        return logits.masked_fill(mask_idx, -math.inf)\n\n    return logits_processor\n</code></pre>"},{"location":"api/samplers/#outlines.samplers.rescale_logits","title":"<code>rescale_logits(temperature)</code>","text":"<p>Build a function that rescales the token probabilities exponentially.</p>"},{"location":"api/samplers/#outlines.samplers.rescale_logits--parameters","title":"Parameters","text":"<p>temperature     The value by which we rescale the logits.</p> Source code in <code>outlines/samplers.py</code> <pre><code>def rescale_logits(temperature: float) -&gt; Callable[[torch.Tensor], torch.Tensor]:\n    \"\"\"Build a function that rescales the token probabilities exponentially.\n\n    Parameters\n    ----------\n    temperature\n        The value by which we rescale the logits.\n\n    \"\"\"\n\n    if not isinstance(temperature, float) or temperature &lt; 0.0:\n        raise ValueError(\n            f\"`temperature` must be a strictly negative floating point number, got {temperature} instead.\"\n        )\n    elif temperature == 0.0:\n        raise ValueError(\n            \"Please use the greedy sampler instead of setting the temperature to 0.\"\n        )\n\n    def logits_processor(logits: torch.Tensor) -&gt; torch.Tensor:\n        return logits / temperature\n\n    return logits_processor\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/01/10/roadmap-for-2024/","title":"Roadmap for 2024","text":"<p>Outlines is not even one year old and it's already gone a long way! As we just reached 4000 stars, and before laying out the roadmap for the following year, we would like to pause and thank all of you for supporting us, using and contributing to the library!</p> <p></p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#thoughts","title":"Thoughts","text":"<p>Before delving into the detailed roadmap, let me share a few thoughts and explain the general direction of the library. These thoughts are informed with my multiple interactions with users, either on Twitter or in our Discord server.</p> <p>Outlines currently differentiates itself from other libraries with its efficient JSON- and regex- constrained generation. A user-facing interface for grammar-structured generation (it had been hidden in the repository) was also recently added. But there is much more we can do along these lines. In 2024 will we will keep pushing in the direction of more accurate, faster constrained generation.</p> <p>Outlines also supports many models providers: <code>transformers</code>, <code>mamba</code>, <code>llama.cpp</code> and <code>exllama2</code>. Those integrations represent a lot of maintenance, and we will need to simplify them. For instance, <code>transformers</code> now supports quantized models, and we will soon deprecate the support for <code>autoawq</code> and <code>autogptq</code>. Thanks to a refactor of the library, it is now possible to use our constrained generation method by using logits processor with all other libraries, except <code>mamba</code>. We will look for libraries that provide state-space models and allow to pass a logits processor during inference. We will interface with <code>llama.cpp</code> and <code>exllama2</code> using logits processors.</p> <p>We would like expand our work to the whole sampling layer, and add new sampling methods that should make structured generation more accurate. This means we will keep the <code>transformers</code> integration as it is today and will expand our text generation logic around this library.</p> <p>Making workflows re-usable and easy to share is difficult today. That is why we are big believers in outlines functions. We will keep improving the interface and adding examples.</p> <p>Finally, we want to add a CLI tool, <code>outlines serve</code>. This will allows you to either serve an API that does general constrained generation, or to serve Outlines function.</p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#detailed-roadmap","title":"Detailed roadmap","text":"<p>Here is a more detailed roadmap for the next 12 months. Outlines is a community effort, and we invite you to pick either topic and contribute to the library. I will progressively add related issues in the repository.</p>"},{"location":"blog/2024/01/10/roadmap-for-2024/#many-more-examples-and-tutorials","title":"Many more examples and tutorials","text":"<p>Let's be honest, Outlines is lacking clear and thorough examples. We want to change this!</p> <ul> <li>How does Outlines work? What can you do with it?</li> <li>What can you do with Outlines that is harder or impossible to do with other libraries?</li> <li>How you can perform standard LLM workflows, for instance Chain of Thoughts, Tree of Thoughts, etc?</li> <li>How does Oultines integrates with the larger ecosystem, for instance other libraries like LangChain and LlamaIndex?</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#simplify-the-integrations","title":"Simplify the integrations","text":"<p>We want to keep the current integrations but lower the maintenance cost so we can focus on what we bring to the table.</p> <ul> <li>Deprecate every obsolete integration: <code>transformers</code> has recently integrated <code>autoawq</code> and <code>autogptq</code> for instance. (PR)</li> <li>See if we can integrate to a library that provides state-space models via a logit processing function;</li> <li>Integrate with llama.cpp via a logits processor;</li> <li>Integrate with exllamav2 via a logits processor;</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#push-structured-generation-further","title":"Push structured generation further","text":"<p>We're just getting started!</p> <ul> <li>Improve the performance of existing structured generation algorithms;</li> <li>Improve the correctness of structured generation algorithms;</li> <li>Add ready-to-use grammars in the grammars repository or in a submodule in Outlines.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#keep-developing-outlines-functions","title":"Keep developing Outlines functions","text":"<p>Functions are awesome, use them!</p> <ul> <li>Implement a CLI <code>outlines serve</code> that allows to serve Outlines functions locally;</li> <li>Add more functions to the functions repository.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#serve-structured-generation","title":"Serve structured generation","text":"<p>We want to make it easier to serve structured generation and outlines functions.</p> <ul> <li>Implement the outlines serve CLI <code>outlines serve</code></li> <li>Serve local APIs that perform structured generation;</li> <li>Serve Outlines functions.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#improve-the-generation-layer","title":"Improve the generation layer","text":"<ul> <li>Use <code>transformers</code>'s private API to prepare inputs for generation inside the <code>Transformers</code> class;</li> <li>Support successions of model generation and text infilling for methods like Beam Search and SMC;</li> <li>Differentiate by adding new caching methods: attention sink, trie-based caching, etc;</li> <li>Differentiate by implementing SMC;</li> <li>Implement Beam Search;</li> <li>Add token healing.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#a-more-seamless-integration-with-openai","title":"A more seamless integration with OpenAI","text":"<ul> <li>Provide the same user interface for OpenAI and open source models so they are easily interchangeable;</li> <li>Integrate the function calling API.</li> </ul>"},{"location":"blog/2024/01/10/roadmap-for-2024/#last-word","title":"Last word","text":"<p>This roadmap was influenced by the expressed interests of the community. If it doesn't reflect your needs please come and share your experience with us.</p>"},{"location":"community/","title":"Community","text":"<p>Outlines exists for a community of users who believe software doesn't need to be complicated. Who share the same passion for Large Language Models but don't want to compromise on robustness. Together, we are bringing these powerful models back to the world of software.</p>"},{"location":"community/#connect-on-discord","title":"Connect on Discord","text":"<p>The Outlines community lives on our Discord server. There you can ask questions, share ideas or just chat with people like you. Don't be a stranger and join us.</p>"},{"location":"community/contribute/","title":"Contribute","text":""},{"location":"community/contribute/#what-contributions","title":"What contributions?","text":"<ul> <li>New features. Please start a new discussion, or come chat with us beforehand!</li> <li>Bug reports with a minimum working examples in the issue tracker</li> <li>Bug fixes are wonderful.</li> <li>Documentation are very valuable to us! The community will be forever grateful.</li> </ul> <p>Note that the issue tracker is only intended for actionable items. In doubt, open a discussion or come talk to us.</p>"},{"location":"community/contribute/#how-to-contribute","title":"How to contribute?","text":""},{"location":"community/contribute/#setup","title":"Setup","text":"<p>First, fork the repository on GitHub and clone the fork locally:</p> <pre><code>git clone git@github.com/YourUserName/outlines.git\ncd outlines\n</code></pre> <p>Create a new virtual environment. If you are using conda:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>If you are using venv:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>Then install the dependencies in editable mode, and install the pre-commit hooks:</p> <pre><code>pip install -e .[test]\npre-commit install\n</code></pre>"},{"location":"community/contribute/#developing-serve-endpoint-via-docker","title":"Developing Serve Endpoint Via Docker","text":"<pre><code>docker build -t outlines-serve .\ndocker run -p 8000:8000 outlines-serve --model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n</code></pre> <p>This builds <code>outlines-serve</code> and runs on <code>localhost:8000</code> with the model <code>Mistral-7B-Instruct-v0.2</code></p>"},{"location":"community/contribute/#before-pushing-your-code","title":"Before pushing your code","text":"<p>Run the tests:</p> <pre><code>pytest\n</code></pre> <p>And run the code style checks:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"community/contribute/#performance-testing","title":"Performance testing","text":"<p>Run benchmark tests:</p> <pre><code>pytest --benchmark-only\n</code></pre> <p>(other pytest-benchmark command line options)</p>"},{"location":"community/contribute/#open-a-pull-request","title":"Open a Pull Request","text":"<p>Create a new branch on your fork, commit and push the changes:</p> <pre><code>git checkout -b new-branch\ngit add .\ngit commit -m \"Changes I made\"\ngit push origin new-branch\n</code></pre> <p>Then you can open a pull request on GitHub. It should prompt you to do so. Every subsequent change that you make on your branch will update the pull request.</p> <p>Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback. If you need help, come tell us on Discord.</p>"},{"location":"community/feedback/","title":"Feedback","text":"<p>If Outlines has been helpful to you, let us know on Discord or give us a shoutout on Twitter! It's always heartwarming \u2764\ufe0f</p> <p> <p></p> <p>Most underrated Github Repo in AI + LLM JSON guided Generation: https://t.co/lSB8KIet1H</p>\u2014 \ud83c\udf99Jean-Louis Queguiner (@JiliJeanlouis) December 18, 2023 <p>Nice and useful. https://t.co/LX72AE0lgt</p>\u2014 Dan Roy (@roydanroy) August 15, 2023 <p>HUGE dub for open source AI https://t.co/bYKuiEUZ1j</p>\u2014 kenneth \ud83d\udd87 (@k3nnethfrancis) August 15, 2023 <p>This is amazing - glad to see more outp guidance modules! Will try this out soon I'm wondering how they translate from regex automatons to token boundariesAlso why Open Source will succeed. Even today I don't see any guided output functionality from the big providers. https://t.co/Ity2H25Klf</p>\u2014 Hrishi (@hrishioa) August 14, 2023 <p>Outlines \u3030\ufe0f- a library to help LLM developers guide text generation in a fast and reliable way.\"Provides generation methods that guarantee that the output will match a regular expressions, or follow a JSON schema.\"Need to check this out. Reliable JSON output is a common use\u2026 pic.twitter.com/Bkbh8vKogN</p>\u2014 elvis (@omarsar0) August 14, 2023 <p>Woah this is cool! Makes open source models more usable.Give any LLM Function Call capability (and more) with Outlines: https://t.co/PtPykR5ZGR https://t.co/RRQjWHnIxv pic.twitter.com/BwNnH8SMwv</p>\u2014 Yohei (@yoheinakajima) August 14, 2023 <p>This is awesome! Being able to guarantee the output's structure unblocks so many applications. This is a great milestone and a fundamental building block for more advanced AI apps. https://t.co/WdwMOc7hE8</p>\u2014 Guilherme Castro (@skastr052) August 15, 2023 <p>Juggling with the unpredictable outputs of ChatGPT API lately while building my product. \ud83d\ude13 Tried prompt engineering to channel its wisdom into a neat JSON, but it's like asking a cat to fetch. \ud83d\udc31Luckily, stumbled upon \"Outlines\" \u2013 looks like a promising way to tame the LLM\u2026 pic.twitter.com/oYQ6q8exAS</p>\u2014 Charlie (@14435635Sun) August 15, 2023 <p>A complex system of LLM input-outputs interacting with non-LLM agents and models benefits immeasurably from structured outputs. The outlines package saves so much time, https://t.co/NhVQ6NpKDR</p>\u2014 Amir Sani (@amirsani) November 26, 2023"},{"location":"community/feedback/#let-us-know","title":"Let us know!","text":"<p>We highly value the insights of our users, and we would love to hear from you. If you are using Outlines for your projects and would like to share your experience with us, let's connect:</p> <ul> <li>What are you building with it?</li> <li>What do you like about it?</li> <li>What challenges are you facing?</li> <li>What do you think could be improved?</li> </ul> <p>To schedule an appointment follow this link. This is exclusively intended to share your experience, please go on Discord or GitHub for support.</p>"},{"location":"cookbook/","title":"Examples","text":"<ul> <li>Classification: Classify customer requests.</li> <li>Named Entity Extraction: Extract information from pizza orders.</li> <li>Dating Profile: Build dating profiles from descriptions using prompt templating and JSON-structured generation.</li> <li>Chain Of Density: Summarize documents using chain of density prompting and JSON-structured generation.</li> <li>Playing Chess: Make Mistral-7B play chess against itself using regex-structured generation.</li> </ul>"},{"location":"cookbook/chain_of_density/","title":"Summarize documents using Chain of Density prompting","text":"<p>A good summary should be informative, concise and clear. While large language models are generally good at summarizing documents, their summaries tend to be long and contain redundant information; their information density tends to be on the lower end. This is where chain of Density, a new prompting technique, comes in. In this example we will show how one can implement chain of density with a few lines of code using Outlines, leveraging both Outline's prompt templating and its structured generation capabilities.</p> <p>The article we will try to summarize is the first three paragraphs of the Alan Turing page on Wikipedia:</p> <pre><code>article = \"\"\"\nAlan Mathison Turing OBE FRS (/\u02c8tj\u028a\u0259r\u026a\u014b/; 23 June 1912 \u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[5] Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[6][7][8] He is widely considered to be the father of theoretical computer science and artificial intelligence.[9]\n\nBorn in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes\u2013no questions can never be answered by computation. He defined a Turing machine and proved that the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.[10][11]\n\nAfter the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[12] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis[1] and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.[13]\n\"\"\"\n</code></pre>"},{"location":"cookbook/chain_of_density/#how-chain-of-density-works","title":"How Chain Of Density works","text":"<p>Chain Of Density starts with asking the model to generate a first long and non-specific summary. Then it asks the model to generate 4 extra summaries by proceeding in the following way:</p> <ol> <li>Identify 1-3 entities missing in the previous summary;</li> <li>Add all entities marked as missing in the previous step, while not dropping entities;</li> <li>Make the summary more concise;</li> </ol> <p>The prompt also asks the model to return a list of JSON objects that contain the missing entities and the new summary. This is where structured generation will come in handy :) The paper provides the prompt and an example:</p> <p></p> <p>We can now implement the prompt provided in the paper:</p> <pre><code>import outlines\n\n@outlines.prompt\ndef chain_of_density(article):\n    \"\"\"Article: {{ article }}\n\n    You will generate increasingly concise, entity-dense summaries of the above Article.\n\n    Repeat the following 2 steps 5 times.\n\n    Step 1. Identify 1-3 informative Entities (\"; \" delimited) from the Article which are missing from the previously generated summary.\n    Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.\n\n    A Missing Entity is:\n    - Relevant: to the main story.\n    - Specific: descriptive yet concise (5 words or fewer).\n    - Novel: not in the previous summary.\n    - Faithful: present in the Article.\n    - Anywhere: located anywhere in the Article.\n\n    Guidelines:\n    - The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~80 words.\n    - Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n    - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n    - Missing entities can appear anywhere in the new summary.\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n\n    Remember, use the exact same number of words for each summary.\n\n    Answer in JSON. The JSON should be a a dictionary with key \"summaries\" that contains a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n    \"\"\"\n</code></pre> Note <p>Note that we modified the prompt slightly so it returns a JSON object that contains the summaries, instead of a list of summaries.</p>"},{"location":"cookbook/chain_of_density/#outlines-implementation","title":"Outlines implementation","text":"<p>We will use Outline's JSON-structured generation to ensure that the model's output is consistent with the format specified in the prompt. We start with defining the JSON objects that the model is asked to return using Pydantic. One JSON object that contains a list of <code>Summary</code> objects that contain the missing entities and new summary:</p> <pre><code>from pydantic import BaseModel, conlist\n\nclass Summary(BaseModel):\n    missing_entities: str\n    denser_summary: str\n\nclass Summaries(BaseModel):\n    summaries: conlist(Summary, max_length=5, min_length=5)\n</code></pre> <p>We now generate the prompt by passing the article we want to summarize to the template. We load a quantized version of Mistral-7B using the AutoAWQ library, and then use JSON-structured generation to generate the summaries:</p> <pre><code>model = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\")\n\nprompt = chain_of_density(article)\nresult = outlines.generate.json(model, Summaries)(prompt)\n</code></pre> <p>We can now check the results:</p> <pre><code>print(result.model_dump())\n# {'summaries': [\n#     {\n#       'missing_entities': 'English mathematician, cryptanalyst, philosopher',\n#       'denser_summary': 'Alan Mathison Turing was an English mathematician, cryptanalyst, philosopher.'\n#     },\n#     {\n#       'missing_entities': '',\n#       'denser_summary': \"Alan Mathison Turing was an English mathematician who was a crucial figure in WW2's Bletchley Park codebreaking centre and designed one of the first computers.\"\n#     },\n#     {\n#       'missing_entities': 'cryptanalyst, studied, biology, father',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and contributed to mathematical biology.'\n#     },\n#     {\n#       'missing_entities': 'biology, morphogenesis, chemical',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, studied theoretical computer science, and predicted chemical reactions in morphogenesis.\n#     '},\n#     {\n#       'missing_entities': '',\n#       'denser_summary': 'Alan Mathison Turing was an English cryptanalyst, developed computer science, and made strides in mathematical biology research.'\n#       }\n# ]}\n</code></pre> <p>Not bad, considering we used a smallish model to generate the summary! Chain of Density seems to be a very effective prompting technique to generate dense summaries, even with small quantized models. Its implementation in Outlines is also very short.</p> <p>Note that this is the first article I tried and it worked out of the box. Try it out on other articles, and please share the results on Twitter, or by opening a new discussion on the Outlines repository!</p>"},{"location":"cookbook/classification/","title":"Classification","text":"<p>Classification is a classic problem in NLP and finds many applications: spam detection, sentiment analysis, triaging of incoming requests, etc. We will use the example of a company that wants to sort support requests between those that require immediate attention (<code>URGENT</code>), those that can wait a little (<code>STANDARD</code>). You could easily extend the example by adding new labels.</p> <p>This tutorial shows how one can implement multi-label classification using Outlines. We will use two functionalities of the library: <code>generate.choice</code> and <code>generate.json</code>.</p> <p>As always, we start with initializing the model. Since we are GPU poor we will be using a quantized version of Mistal-7B-v0.1:</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre> <p>We will use the following prompt template:</p> <pre><code>@outlines.prompt\ndef customer_support(request):\n    \"\"\"You are an experienced customer success manager.\n\n    Given a request from a client, you need to determine when the\n    request is urgent using the label \"URGENT\" or when it can wait\n    a little with the label \"STANDARD\".\n\n    # Examples\n\n    Request: \"How are you?\"\n    Label: STANDARD\n\n    Request: \"I need this fixed immediately!\"\n    Label: URGENT\n\n    # TASK\n\n    Request: {{ request }}\n    Label: \"\"\"\n</code></pre>"},{"location":"cookbook/classification/#choosing-between-multiple-choices","title":"Choosing between multiple choices","text":"<p>Outlines provides a shortcut to do multi-label classification, using the <code>outlines.generate.choice</code> function to initialize a generator. Outlines uses multinomial sampling by default, here we will use the greedy sampler to get the label with the highest probability:</p> <p><pre><code>from outlines.generate.samplers import greedy\n\ngenerator = outlines.generate.choice(model, [\"URGENT\", \"STANDARD\"], sampler=greedy)\n</code></pre> Outlines supports batched requests, so we will pass two requests to the model:</p> <pre><code>requests = [\n    \"My hair is one fire! Please help me!!!\",\n    \"Just wanted to say hi\"\n]\n\nprompts = [customer_support(request) for request in requests]\n</code></pre> <p>We can now asks the model to classify the requests:</p> <pre><code>labels = generator(prompts)\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre> <p>Now, you might be in a hurry and don't want to wait until the model finishes completion. After all, you only need to see the first letter of the response to know whether the request is urgent or standard. You can instead stream the response:</p> <pre><code>tokens = generator.stream(prompts)\nlabels = [\"URGENT\" if \"U\" in token else \"STANDARD\" for token in next(tokens)]\nprint(labels)\n# ['URGENT', 'STANDARD']\n</code></pre>"},{"location":"cookbook/classification/#using-json-structured-generation","title":"Using JSON-structured generation","text":"<p>Another (convoluted) way to do multi-label classification is to JSON-structured generation in Outlines. We first need to define our Pydantic schema that contains the labels:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\n\nclass Label(str, Enum):\n    urgent = \"URGENT\"\n    standard = \"STANDARD\"\n\n\nclass Classification(BaseModel):\n    label: Label\n</code></pre> <p>and we can use <code>generate.json</code> by passing this Pydantic model we just defined, and call the generator:</p> <pre><code>generator = outlines.generate.json(model, Classification, sampler=greedy)\nlabels = generator(prompts)\nprint(labels)\n# [Classification(label=&lt;Label.urgent: 'URGENT'&gt;), Classification(label=&lt;Label.standard: 'STANDARD'&gt;)]\n</code></pre>"},{"location":"cookbook/dating_profiles/","title":"Generate a synthetic dating profile from a description","text":"<p>In this example we will see how we can use Outlines to generate synthetic data for a dating application. This example was originally contributed by Vibhor Kumar.</p> <pre><code>from dataclasses import dataclass\nfrom enum import Enum\n\nimport torch\nimport transformers\nfrom pydantic import BaseModel, conlist, constr\n\nimport outlines\n</code></pre>"},{"location":"cookbook/dating_profiles/#defining-the-profile-with-pydantic","title":"Defining the profile with Pydantic","text":"<p>Here a dating profile will consist in a biography, a job, a list of interests and two question-answer pairs. The questions are written in advance by the team, and the users are asked to provide an answer:</p> <pre><code>class QuestionChoice(str, Enum):\n    A = \"The key to my heart is\"\n    B = \"The first item on my bucket list is\"\n    C = \"Perks of dating me\"\n    D = \"Message me if you also love\"\n    E = \"People would describe me as\"\n    F = \"I can beat you in a game of\"\n\n@dataclass\nclass QuestionAnswer:\n    question: QuestionChoice\n    answer: str\n</code></pre> <p>Users need to provide a short biography, with a minimum of 10 and a maximum of 300 characters. The application also limits job descriptions to 50 characters. In addition to the question-answer pairs, the user is required to provide a list of between 1 and 5 interests:</p> <pre><code>class DatingProfile(BaseModel):\n    bio: constr(str, min_length=10, max_length=300)\n    job: constr(str, max_lengt=50)\n    interests: conlist(str, min_length=1, max_length=5)  # type: ignore\n    qna1: QuestionAnswer\n    qna2: QuestionAnswer\n</code></pre>"},{"location":"cookbook/dating_profiles/#prompt-template-and-examples","title":"Prompt template and examples","text":"<p>We will ask the model to generate profiles from a high-level description:</p> <pre><code>@dataclass\nclass Example:\n    description: str\n    profile: DatingProfile\n</code></pre> <p>We will use Outlines' prompt templating abilities to generate the prompt for us. This help clearly separate the general prompting logic from what is specific to an example.</p> <pre><code>@outlines.prompt\ndef dating_profile_prompt(description: str, examples: list[Example]):\n    \"\"\"\n    You are a world-renowned matchmaker who understands the modern dating\n    market. Your job is to generate dating app profiles for male clients\n    interested in women based on a provided description. The profiles should be\n    authentic, show off their strengths, and maximize their likelihood of\n    getting matches on dating apps.  Here are some examples of past clients that\n    you have successfully created profiles for:\n\n    {% for example in examples %}\n    Description:\n    {{ example.description }}\n    Profile:\n    {{ example.profile }}\n    {% endfor %}\n\n    Here is the new client who you need to create a profile for:\n    Description: {{ description }}\n    Profile:\n    \"\"\"\n</code></pre> <p>We will provide the model with several few-shot examples:</p> <pre><code>samples: list[Example] = [\n    Example(\n        description=\"I'm an author and former professional soccer player living in Seattle who publishes popular fiction books. A typical day for me starts by hanging out with my cat, drinking a coffee, and reading as much as I can in a few hours. Then, I'll prepare a quick smoothie before starting to write for a few hours, take a break with soccer or running a few miles, and finally meet friends for dinner at a new, hip restaurant in the evening. Sometimes we go axe-throwing afterwards, or play poker, or watch a comedy show, or visit a dive bar. On my vacations, I travel extensively to countries South America, Europe, and Asia, with the goal of visiting them all!\",\n        profile=DatingProfile(\n            bio=\"Adventurer, dreamer, author, and soccer enthusiast. Life\u2019s too short to waste time so I make the most of each day by exploring new places and playing with my friends on the pitch. What\u2019s your favorite way to get out and have fun?\",\n            job=\"Famous Soccer Player -&gt; Famous Author\",\n            interests=[\"Soccer\", \"Travel\", \"Friends\", \"Books\", \"Fluffy Animals\"],\n            qna1=QuestionAnswer(\n                question=QuestionChoice.B, answer=\"swim in all seven oceans!\"\n            ),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.E,\n                answer=\"fun-loving, adventurous, and a little bit crazy\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my company and build houses for a living. I'm a big fan of the outdoors and love to go hiking, camping, and fishing. I don't like video games, but do like to watch movies. My love language is home-cooked food, and I'm looking for someone who isn't afraid to get their hands dirty.\",\n        profile=DatingProfile(\n            bio=\"If you're looking for a Montana man who loves to get outdoors and hunt, and who's in-tune with his masculinity then I'm your guy!\",\n            job=\"House Construction Manager / Entrepreneur\",\n            interests=[\"Hunting\", \"Hiking\", \"The outdoors\", \"Home-cooked food\"],\n            qna1=QuestionAnswer(question=QuestionChoice.A, answer=\"food made at home\"),\n            qna2=QuestionAnswer(\n                question=QuestionChoice.C,\n                answer=\"having a man in your life who can fix anything\",\n            ),\n        ),\n    ),\n    Example(\n        description=\"I run my own Youtube channel with 10M subscribers. I love working with kids, and my audience skews pretty young too. In my free time, I play Fortnite and Roblox. I'm looking for someone who is also a gamer and likes to have fun. I'm learning Japanese in my free time as well as how to cook.\",\n        profile=DatingProfile(\n            bio=\"Easy on the eyes (find me on Youtube!) and great with kids. What more do you need?\",\n            job=\"Youtuber 10M+ subscribers\",\n            interests=[\"Kids\", \"Gaming\", \"Japanese\"],\n            qna1=QuestionAnswer(question=QuestionChoice.D, answer=\"anime and gaming!\"),\n            qna2=QuestionAnswer(question=QuestionChoice.F, answer=\"Fortnite, gg ez\"),\n        ),\n    ),\n]\n</code></pre>"},{"location":"cookbook/dating_profiles/#load-the-model","title":"Load the model","text":"<p>We will use Mosaic's MPT-7B model (requires 13GB of GPU memory) which can fit on a single GPU with a reasonable context window. We initialize it with Outlines:</p> <pre><code>config = transformers.AutoConfig.from_pretrained(\n    \"mosaicml/mpt-7b-8k-instruct\", trust_remote_code=True\n)\nconfig.init_device = \"meta\"\nmodel = outlines.models.transformers(\n    model_name=\"mosaicml/mpt-7b-8k-instruct\",\n    device=\"cuda\",\n    model_kwargs={\n        \"config\": config,\n        \"trust_remote_code\": True,\n        \"torch_dtype\": torch.bfloat16,\n        \"device_map\": {\"\": 0},\n    },\n)\n</code></pre>"},{"location":"cookbook/dating_profiles/#json-structured-generation-of-profiles","title":"JSON-structured generation of profiles","text":"<p>We will now generate a dating profile from a textual description of oneself:</p> <pre><code>new_description = \"\"\"I'm a laid-back lawyer who spends a lot of his free-time\ngaming. I work in a corporate office, but ended up here after the start-up  I\ncofounded got acquired, so still play ping pong with my cool coworkers every\nday.  I have a bar at home where I make cocktails, which is great for\nentertaining  friends. I secretly like to wear suits and get a new one tailored\nevery few  months. I also like weddings because I get to wear those suits, and\nit's  a good excuse for a date. I watch the latest series because I'm paying,\nwith my hard-earned money, for every streaming service.\"\"\"\n\nprompt = dating_profile_prompt(new_description, samples)\nprofile = outlines.generate.json(model, DatingProfile)(prompt)\nparsed_profile = DatingProfile.model_validate_json(profile)\n</code></pre>"},{"location":"cookbook/dating_profiles/#results","title":"Results","text":"<p>Here are a couple of results:</p> <pre><code>{\n    \"bio\": \"\"\"I'm an ambitious lawyer with a casual and fashionable style. I love\n    games and sports, but my true passion is preparing refreshing cocktails at\n    home and dressing to the nines at weddings. I'm currently looking for a woman\n    to show a good time to and get a kiss on the opulent suit I just had made.\n    Send resume to this inbox.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Stylish guys\",\n        \"Gaming\",\n        \"Ping pong\",\n        \"Cocktails\",\n        \"Weddings\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"be married and have a family.\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"charming, stylish, and funny.\"\n    }\n}\n</code></pre> <pre><code>{\n    \"bio\": \"\"\"I\u2019m a sexy lawyer with time on my hands. I love to game and\n    play ping pong, but the real reason you should swipe to the right\n    is because I look great in a suit. Who doesn\u2019t love a man in a\n    suit? Just saying. Send me a message if you think it\u2019s time to take\n    your dating life to the next level.\"\"\",\n    \"job\": \"Lawyer\",\n    \"interests\":\n    [\n        \"Gaming\",\n        \"Ping Pong\",\n        \"Tailored Suits\",\n        \"Weddings\",\n        \"Streaming Services\"\n    ],\n    \"qna1\":\n    {\n        \"question\": \"The first item on my bucket list is\",\n        \"answer\": \"simulate space but stay alive for as long as possible\"\n    },\n    \"qna2\":\n    {\n        \"question\": \"People would describe me as\",\n        \"answer\": \"easy-going, a little nerdy but with a mature essence\"\n    }\n}\n</code></pre>"},{"location":"cookbook/extraction/","title":"Named entity extraction","text":"<p>Named Entity Extraction is a fundamental problem in NLP. It involves identifying and categorizing named entities within a document: people, organization, dates, places, etc. It is usually the first step in a more complex NLP worklow. Here we will use the example of a pizza restaurant that receives orders via their website and need to identify the number and types of pizzas that are being ordered.</p> <p>Getting LLMs to output the extracted entities in a structured format can be challenging. In this tutorial we will see how we can use Outlines' JSON-structured generation to extract entities from a document and return them in a valid JSON data structure 100% of the time.</p> <p>As always, we start with initializing the model. We will be using a quantized version of Mistal-7B-v0.1 (we're GPU poor):</p> <pre><code>import outlines\n\nmodel = outlines.models.transformers(\"TheBloke/Mistral-7B-OpenOrca-AWQ\", device=\"cuda\")\n</code></pre> <p>And we will be using the following prompt template:</p> <pre><code>@outlines.prompt\ndef take_order(order):\n    \"\"\"You are the owner of a pizza parlor. Customers \\\n    send you orders from which you need to extract:\n\n    1. The pizza that is ordered\n    2. The number of pizzas\n\n    # EXAMPLE\n\n    ORDER: I would like one Margherita pizza\n    RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n\n    # OUTPUT INSTRUCTIONS\n\n    Answer in valid JSON. Here are the different objects relevant for the output:\n\n    Order:\n        pizza (str): name of the pizza\n        number (int): number of pizzas\n\n    Return a valid JSON of type \"Order\"\n\n    # OUTPUT\n\n    ORDER: {{ order }}\n    RESULT: \"\"\"\n</code></pre> <p>We now define our data model using Pydantic:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\n\nclass Pizza(str, Enum):\n    margherita = \"Margherita\"\n    pepperonni = \"Pepperoni\"\n    calzone = \"Calzone\"\n\nclass Order(BaseModel):\n    pizza: Pizza\n    number: int\n</code></pre> <p>We can now define our generator and call it on several incoming orders:</p> <pre><code>orders = [\n    \"Hi! I would like to order two pepperonni pizzas and would like them in 30mins.\",\n    \"Is it possible to get 12 margheritas?\"\n]\nprompts = [take_order(order) for order in orders]\n\ngenerator = outlines.generate.json(model, Order)\n\nresults = generator(prompts)\nprint(results)\n# [Order(pizza=&lt;Pizza.pepperonni: 'Pepperoni'&gt;, number=2),\n#  Order(pizza=&lt;Pizza.margherita: 'Margherita'&gt;, number=12)]\n</code></pre> <p>There are several ways you could improve this example:</p> <ul> <li>Clients may order several types of pizzas.</li> <li>Clients may order drinks as well.</li> <li>If the pizza place has a delivery service we need to extract the client's address and phone number</li> <li>Clients may specify the time for which they want the pizza. We could then check against a queuing system and reply to them with the estimated delivery time.</li> </ul> <p>How would you change the Pydantic model to account for these use cases?</p>"},{"location":"cookbook/models_playing_chess/","title":"Large language models playing chess","text":"<p>In this example we will make a Phi-2 model play chess against itself. On its own the model easily generates invalid moves, so we will give it a little help. At each step we will generate a regex that only matches valid move, and use it to help the model only generating valid moves.</p>"},{"location":"cookbook/models_playing_chess/#the-chessboard","title":"The chessboard","text":"<p>The game will be played on a standard checkboard. We will use the <code>chess</code> library to track the opponents' moves, and check that the moves are valid.</p> <pre><code>%pip install outlines -q\n%pip install chess -q\n%pip install transformers accelerate einops -q\n\nimport chess\n\nboard = chess.Board(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\")\n</code></pre>"},{"location":"cookbook/models_playing_chess/#the-opponents","title":"The opponents","text":"<p>Phi-2 will be playing against itself:</p> <pre><code>from outlines import models\n\nmodel = models.transformers(\"microsoft/phi-2\")\n</code></pre>"},{"location":"cookbook/models_playing_chess/#a-little-help-for-the-language-model","title":"A little help for the language model","text":"<p>To make sure Phi-2 generates valid chess moves we will use Outline's regex-structured generation. We define a function that takes the current state of the board and returns a regex that matches all possible legal moves:</p> <pre><code>import re\n\ndef legal_moves_regex(board):\n    \"\"\"Build a regex that only matches valid moves.\"\"\"\n    legal_moves = list(board.legal_moves)\n    legal_modes_str = [board.san(move) for move in legal_moves]\n    legal_modes_str = [re.sub(r\"[+#]\", \"\", move) for move in legal_modes_str]\n    regex_pattern = \"|\".join(re.escape(move) for move in legal_modes_str)\n    regex_pattern = f\"{regex_pattern}\"\n    return regex_pattern\n</code></pre>"},{"location":"cookbook/models_playing_chess/#prompting-the-language-model","title":"Prompting the language model","text":"<p>The prompt corresponds to the current state of the board, so we start with:</p> <pre><code>prompt = \"Let's play Chess. Moves: \"\n</code></pre> <p>We update the prompt at each step so it reflects the state of the board after the previous move.</p>"},{"location":"cookbook/models_playing_chess/#lets-play","title":"Let's play","text":"<pre><code>from outlines import generate\n\nboard_state = \" \"\nturn_number = 0\nwhile not board.is_game_over():\n    regex_pattern = legal_moves_regex(board)\n    structured = generate.regex(model, regex_pattern)(prompt + board_state)\n    move = board.parse_san(structured)\n\n    if turn_number % 2 == 0 :  # It's White's turn\n        board_state += board.san(move) + \" \"\n    else:\n        board_state += board.san(move) + \" \" + str(turn_number) + \".\"\n\n    turn_number += 1\n\n    board.push(move)\n\n    print(board_state)\n</code></pre> <p>Interestingly enough, Phi-2 hates capturing.</p> <pre><code> e4 e5 1.Nf3 Ne7 3.b4 Nf5 5.Nc3 Ne7 7.Bb5 a6 9.Na4 b6 11.c3 Nec6 13.c4 a5 15.d4 Qg5 17.Nd2 Bb7 19.dxe5\n</code></pre> <p>This example was originally authored by @903124S in this gist.</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#structured-generation","title":"Structured generation","text":"<p>While LLM capabilities are increasingly impressive, we can make their output more reliable by steering the generation. Outlines thus offers mechanisms to specify high level constraints on text completions by generative language models.</p> <p>Stopping sequence By default, language models stop generating tokens after and  token was generated, or after a set maximum number of tokens. Their output can be verbose, and for practical purposes it is often necessary to stop the generation after a given sequence has been found instead. You can use the stop_at keyword argument when calling the model with a prompt: <pre><code>import outlines.models as models\n\ncomplete = models.openai(\"gpt-3.5-turbo\")\nexpert = complete(\"Name an expert in quantum gravity.\", stop_at=[\"\\n\", \".\"])\n</code></pre>"},{"location":"reference/cfg/","title":"Grammar-structured generation","text":""},{"location":"reference/choices/","title":"Multiple choices","text":"<p>Choice between different options In some cases we know the output is to be chosen between different options. We can restrict the completion\u2019s output to these choices using the is_in keyword argument:</p> <pre><code>import outlines.models as models\n\ncomplete = models.openai(\"gpt-3.5-turbo\")\nanswer = complete(\n    \"Pick the odd word out: skirt, dress, pen, jacket\",\n    is_in=[\"skirt\", \"dress\", \"pen\", \"jacket\"]\n)\n</code></pre>"},{"location":"reference/custom_fsm_ops/","title":"Custom FSM Operations","text":"<p><code>RegexFSM.from_interegular_fsm</code> leverages the flexibility of <code>interegular.FSM</code> to use the available operations in <code>interegular</code>.</p>"},{"location":"reference/custom_fsm_ops/#examples","title":"Examples","text":""},{"location":"reference/custom_fsm_ops/#difference","title":"<code>difference</code>","text":"<p>Returns an FSM which recognises only the strings recognised by the first FSM in the list, but none of the others.</p> <pre><code>list_of_strings_pattern = \"\"\"\\[\"[^\"\\s]*\"(?:,\"[^\"\\s]*\")*\\]\"\"\"\npink_elephant_pattern = \"\"\".*(pink|elephant).*\"\"\"\n\nlist_of_strings_fsm = interegular.parse_pattern(list_of_strings_pattern).to_fsm()\npink_elephant_fsm = interegular.parse_pattern(pink_elephant_pattern).to_fsm()\n\nlist_of_strings_fsm.accepts('[\"a\",\"pink\",\"elephant\"]')\n# True\n\ndifference_fsm = list_of_strings_fsm - pink_elephant_fsm\n\ndifference_fsm_fsm.accepts('[\"a\",\"pink\",\"elephant\"]')\n# False\ndifference_fsm_fsm.accepts('[\"a\",\"blue\",\"donkey\"]')\n# True\n</code></pre>"},{"location":"reference/custom_fsm_ops/#union","title":"<code>union</code>","text":"<p>Returns a finite state machine which accepts any sequence of symbols that is accepted by either self or other.</p> <pre><code>list_of_strings_pattern = \"\"\"\\[\"[^\"\\s]*\"(?:,\"[^\"\\s]*\")*\\]\"\"\"\ntuple_of_strings_pattern = \"\"\"\\(\"[^\"\\s]*\"(?:,\"[^\"\\s]*\")*\\)\"\"\"\n\nlist_of_strings_fsm = interegular.parse_pattern(list_of_strings_pattern).to_fsm()\ntuple_of_strings_fsm = interegular.parse_pattern(tuple_of_strings_pattern).to_fsm()\n\nlist_of_strings_fsm.accepts('(\"a\",\"pink\",\"elephant\")')\n# False\n\nunion_fsm = list_of_strings_fsm|tuple_of_strings_fsm\n\nunion_fsm.accepts('[\"a\",\"pink\",\"elephant\"]')\n# True\nunion_fsm.accepts('(\"a\",\"blue\",\"donkey\")')\n# True\n</code></pre>"},{"location":"reference/custom_fsm_ops/#intersection","title":"<code>intersection</code>","text":"<p>Returns an FSM which accepts any sequence of symbols that is accepted by both of the original FSMs.</p> <pre><code>list_of_strings_pattern = \"\"\"\\[\"[^\"\\s]*\"(?:,\"[^\"\\s]*\")*\\]\"\"\"\npink_elephant_pattern = \"\"\".*(pink|elephant).*\"\"\"\n\nlist_of_strings_fsm = interegular.parse_pattern(list_of_strings_pattern).to_fsm()\npink_elephant_fsm = interegular.parse_pattern(pink_elephant_pattern).to_fsm()\n\nlist_of_strings_fsm.accepts('[\"a\",\"blue\",\"donkey\"]')\n# True\n\nintersection_fsm = list_of_strings_fsm &amp; pink_elephant_fsm\n\nintersection_fsm.accepts('[\"a\",\"pink\",\"elephant\"]')\n# True\nintersection_fsm.accepts('[\"a\",\"blue\",\"donkey\"]')\n# False\n</code></pre> <p>There are more operations available, we refer to https://github.com/MegaIng/interegular/blob/master/interegular/fsm.py.</p>"},{"location":"reference/custom_fsm_ops/#loading-custom-fsm","title":"Loading Custom FSM","text":"<pre><code>import outlines\n\ngenerator = outlines.generate.fsm(model, custom_fsm)\n\nresponse = generator(prompt)\n</code></pre>"},{"location":"reference/functions/","title":"Outlines functions","text":""},{"location":"reference/json/","title":"Make the LLM follow a JSON Schema","text":"<p>Outlines can make any open source model return a JSON object that follows a structure that is specified by the user. This is useful whenever we want the output of the model to be processed by code downstream: code does not understand natural language but rather the structured language it has been programmed to understand.</p> <p>There are mostly two reasons why someone would want to get an output formatted as JSON from a LLM:</p> <ol> <li>Parse the answer (e.g. with Pydantic), store it somewhere, return it to a user, etc.</li> <li>Call a function with the result</li> </ol> <p>Outlines has you covered in both cases! Indeed, to define the structure of the JSON you want the model to follow you can either provide a Pydantic model, or a function. No need to duplicate code!</p>"},{"location":"reference/json/#using-pydantic","title":"Using Pydantic","text":"<p>Outlines can infer the structure of the output from a Pydantic model. The result is an instance of the model that contains the values returned by the LLM:</p> <pre><code>from pydantic import BaseModel\n\nfrom outlines import models\nfrom outlines import text\n\n\nclass User(BaseModel):\n    name: str\n    last_name: str\n    id: int\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = text.generate.json(model, User)\nresult = generator(\"Create a user profile with the fields name, last_name and id\")\nprint(result)\n# User(name=\"John\", last_name=\"Doe\", id=11)\n</code></pre> <p>JSON and whitespaces</p> <p>By default Outlines lets model choose the number of linebreaks and white spaces used to structure the JSON. Small models tend to struggle with this, in which case we recommend to set the value of the parameter <code>whitespace_pattern</code> to the empty string:</p> <pre><code>generator = text.generate.json(model, User, whitespace_pattern=\"\")\n</code></pre>"},{"location":"reference/json/#from-a-functions-signature","title":"From a function's signature","text":"<p>Outlines can infer the structure of the output from the signature of a function. The result is a dictionary, and can be passed directly to the function using the usual dictionary expansion syntax <code>**</code>:</p> <pre><code>from outlines import models\nfrom outlines import text\n\ndef add(a: int, b: int):\n    return a + b\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = text.generate.json(model, add)\nresult = generator(\"Return two integers named a and b respectively. a is odd and b even.\")\n\nprint(add(**result))\n# 3\n</code></pre> <p>A great advantage of passing functions directly to specify the structure is that the structure of the LLM will change with the function's definition. No need to change the code at several places!</p>"},{"location":"reference/prompting/","title":"Prompt templating","text":"<p>Outlines provides a powerful domain-specific language to write and manage prompts, via what we call prompt functions.  Prompt functions are Python functions that contain a template for the prompt in their docstring, and their arguments correspond to the variables used in the prompt. When called, a prompt function returns the template rendered with the values of the arguments.</p> <p>The aim of prompt functions is to solve several recurrent problems with prompting:</p> <ol> <li>Building complex prompts quickly leads to messy code. This problem has    already been solved in the web development community by using templating, so    why not use it here?</li> <li>Composing prompts is difficult. Why not just compose functions?</li> <li>Separating prompts from code. Encapsulation in functions allows a clean    separation between prompts and code. Moreover, like any function, prompt    functions can be imported from other modules.</li> </ol> <p>Outlines uses the Jinja templating engine to render prompts, which allows to easily compose complex prompts.</p> <p>Prompt rendering</p> <p>Prompt functions are opinionated when it comes to prompt rendering. These opinions are meant to avoid common prompting errors, but can have unintended consequences if you are doing something unusual. We advise to always print the prompt before using it. You can also read the reference section if you want to know more.</p>"},{"location":"reference/prompting/#your-first-prompt","title":"Your first prompt","text":"<p>The following snippet showcases a very simple prompt. The variables between curly brackets <code>{{  }}</code> are placeholders for the values of the arguments you will pass to the prompt function.</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef greetings(name, question):\n    \"\"\"Hello, {{ name }}!\n    {{ question }}\n    \"\"\"\n\nprompt = greetings(\"user\", \"How are you?\")\nprint(prompt)\n</code></pre> <pre><code>Hello, user!\nHow are you?\n</code></pre> <p>If a variable is missing in the function's arguments, Jinja2 will throw an <code>UndefinedError</code> exception:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef greetings(name):\n    \"\"\"Hello, {{ surname }}!\"\"\"\n\nprompt = greetings(\"user\")\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 9, in &lt;module&gt;\n  File \"/home/remi/projects/normal/outlines/outlines/prompts.py\", line 38, in __call__\n      return render(self.template, **bound_arguments.arguments)\n  File \"/home/remi/projects/normal/outlines/outlines/prompts.py\", line 213, in render\n      return jinja_template.render(**values)\n  File \"/home/remi/micromamba/envs/outlines/lib/python3.9/site-packages/jinja2/environment.py\", line 1301, in render\n      self.environment.handle_exception()\n  File \"/home/remi/micromamba/envs/outlines/lib/python3.9/site-packages/jinja2/environment.py\", line 936, in handle_exception\n      raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 1, in top-level template code\n  jinja2.exceptions.UndefinedError: 'surname' is undefined\n</code></pre>"},{"location":"reference/prompting/#importing-prompt-functions","title":"Importing prompt functions","text":"<p>Prompt functions are functions, and thus can be imported from other modules:</p> prompts.pygenerate.pyOutput <pre><code>import outlines\n\n@outlines.prompt\ndef greetings(name, question):\n    \"\"\"Hello, {{ name }}!\n    {{ question }}\n    \"\"\"\n</code></pre> <pre><code>from .prompts import greetings\n\nprompt = greetings(\"John Doe\", \"How are you today?\")\n</code></pre> <pre><code>Hello, John Doe!\nHow are you today?\n</code></pre>"},{"location":"reference/prompting/#few-shot-prompting","title":"Few-shot prompting","text":"<p>Few-shot prompting can lead to messy code. Prompt functions allow you to loop over lists or dictionaries from the template. In the following example we demonstrate how we can generate a prompt by passing a list of dictionaries with keys <code>question</code> and <code>answer</code> to the prompt function:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef few_shots(instructions, examples, question):\n    \"\"\"{{ instructions }}\n\n    Examples\n    --------\n\n    {% for example in examples %}\n    Q: {{ example.question }}\n    A: {{ example.answer }}\n\n    {% endfor %}\n    Question\n    --------\n\n    Q: {{ question }}\n    A:\n    \"\"\"\n\ninstructions = \"Please answer the following question following the examples\"\nexamples = [\n    {\"question\": \"2+2=?\", \"answer\":4},\n    {\"question\": \"3+3=?\", \"answer\":6}\n]\nquestion = \"4+4 = ?\"\n\nprompt = few_shots(instructions, examples, question)\nprint(prompt)\n</code></pre> <pre><code>Please answer the following question following the examples\n\nExamples\n--------\n\nQ: 2+2=?\nA: 4\n\nQ: 3+3=?\nA: 6\n\nQuestion\n--------\n\nQ: 4+4 = ?\nA:\n</code></pre>"},{"location":"reference/prompting/#conditionals-filters-etc","title":"Conditionals, filters, etc.","text":"<p>Jinja2 has many features beyond looping that are not described here: conditionals, filtering, formatting, etc. Please refer to the Jinja documentation for more information about the syntax of the templating language. The Jinja syntax is powerful, and we recommend you take some time to read their documentation if you are building complex prompts.</p>"},{"location":"reference/prompting/#tools","title":"Tools","text":"<p>Several projects (e.g.Toolformer, ViperGPT, AutoGPT, etc.) have shown that we can \"teach\" language models to use external functions by describing what these functions do in the prompt. In these projects the same information is often repeated twice: the function implementation, name, docstring, or arguments are copy-pasted in the prompt. This is cumbersome and error prone; you can directly pull this information from within an Outlines prompt function:</p> CodeOutput <pre><code>import outlines\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n\n@outlines.prompt\ndef tool_prompt(question, tool):\n    \"\"\"{{ question }}\n\n    COMMANDS\n    1. {{ tool | name }}: {{ tool | description }}, args: {{ tool | args }}\n\n    {{ tool | source }}\n    \"\"\"\n\nprompt = tool_prompt(\"Can you do something?\", my_tool)\nprint(prompt)\n</code></pre> <pre><code>Can you do something?\n\nCOMMANDS\n1. my_tool: Tool description, args: arg1:str, arg2:int\n\ndef my_tool(arg1: str, arg2: int):\n    \"\"\"Tool description.\n\n    The rest of the docstring\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/prompting/#json-response-format","title":"JSON response format","text":"<p>To build reliable chains with language models we often need to instruct them the format in which we would like them to return their response.</p> <p>Without prompt templating, the information is repeated twice between creating the parsing function (e.g. a Pydantic model), and writing the desired schema in the prompt. This can lead to errors that are hard to debug.</p> <p>Outlines allows you to directly pull the JSON schema of a pydantic model, or pretty print a dictionary from within an Outlines prompt function</p> CodeOutput <pre><code>from pydantic import BaseModel, Field\n\nimport outlines\n\nclass MyResponse(BaseModel):\n    field1: int = Field(description=\"an int\")\n    field2: str\n\n@outlines.prompt\ndef my_prompt(response_model):\n    \"\"\"{{ response_model | schema }}\"\"\"\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"an int\",\n#   \"field2\": \"&lt;field2&gt;\"\n# }\n</code></pre> <pre><code>response = {\n    \"field1\": \"&lt;field1&gt;\",\n    \"field2\": \"a string\"\n}\n\nmy_prompt(MyResponse)\n# {\n#   \"field1\": \"&lt;field1&gt;\",\n#   \"field2\": \"a string\"\n# }\n</code></pre>"},{"location":"reference/prompting/#formatting-conventions","title":"Formatting conventions","text":"<p>Prompt functions are opinionated when it comes to rendering, and these opinions are meant to avoid prompting mistakes and help with formatting.</p>"},{"location":"reference/prompting/#whitespaces","title":"Whitespaces","text":"<p>If you have experience working with strings between triple quotes you know that indenting has an influence on the string's formatting. Prompt functions adopt a few conventions so you don't have to think about indents when writing prompt.</p> <p>First, whether you start the prompt right after the triple quotes or on the line below does not matter for formatting:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef prompt1():\n    \"\"\"My prompt\n    \"\"\"\n\n@outlines.prompt\ndef prompt2():\n    \"\"\"\n    My prompt\n    \"\"\"\n\nprint(prompt1())\nprint(prompt2())\n</code></pre> <pre><code>My prompt\nMy prompt\n</code></pre> <p>Indentation is relative to the second line of the docstring, and leading spaces are removed:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef example1():\n    \"\"\"First line\n    Second line\n    \"\"\"\n\n@outlines.prompt\ndef example2():\n    \"\"\"\n      Second line\n      Third line\n    \"\"\"\n\n@outlines.prompt\ndef example3():\n    \"\"\"\n      Second line\n        Third line\n    \"\"\"\n\nprint(example1())\nprint(example2())\nprint(example3())\n</code></pre> <pre><code>First line\nSecond line\n\nSecond line\nThird line\n\nSecond line\n  Third line\n</code></pre> <p>Trailing whitespaces are not removed, unless they follow a linebreak symbol <code>\\</code> (see linebreaks).</p>"},{"location":"reference/prompting/#linebreaks","title":"Linebreaks","text":"<p>You can use the backslash <code>\\</code> to break a long line of text. It will render as a single line:</p> CodeOutput <pre><code>import outlines\n\n@outlines.prompt\ndef example():\n   \"\"\"\n   Break in \\\n   several lines \\\n   But respect the indentation\n       on line breaks.\n   And after everything \\\n   Goes back to normal\n   \"\"\"\n\nprint(example())\n</code></pre> <pre><code>Break in several lines But respect the indentation\n    on line breaks.\nAnd after everything Goes back to normal\n</code></pre>"},{"location":"reference/regex/","title":"Regular expressions","text":""},{"location":"reference/samplers/","title":"Samplers","text":"<p>Outlines offers different sequence sampling algorithms, and we will integrate more in the future. You can read this blog post for an overview of the different sampling algorithm.</p>"},{"location":"reference/samplers/#multinomial-sampling","title":"Multinomial sampling","text":"<p>Outlines defaults to the multinomial sampler without top-p or top-k sampling, and temperature equal to 1. Not specifying a sampler is equivalent to:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-0.1\")\nsampler = samplers.multinomial()\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre> <p>You can ask the generator to take multiple samples by passing the number of samples when initializing the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-0.1\")\nsampler = samplers.multinomial(3)\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# [4, 4, 4]\n</code></pre> <p>If you ask multiple samples for a batch of prompt the returned array will be of shape <code>(num_samples, num_batches)</code>:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-0.1\")\nsampler = samplers.multinomial(3)\n\ngenerator = generate.text(model, sampler)\nanswer = generator([\"What is 2+2?\", \"What is 3+3?\"])\n\nprint(answer)\n# [[4, 4, 4], [6, 6, 6]]\n</code></pre>"},{"location":"reference/samplers/#top-k-sampling","title":"Top-k sampling","text":"<p>You can ask Outlines to only consider the top-k logits at each step by specifying the value of the <code>top-k</code> keyword argument when initializing the sampler.</p> <pre><code>sampler = samplers.multinomial(3, top_k=10)\n</code></pre>"},{"location":"reference/samplers/#top-p-sampling","title":"Top-p sampling","text":"<p>You can ask Outlines to only consider the highest probability tokens such that their cumulative probability is greater than a threshold <code>p</code>. Specify the <code>top_p</code> keyword argument when initializing the sampler:</p> <pre><code>sampler = samplers.multinomial(3, top_p=0.95)\n</code></pre>"},{"location":"reference/samplers/#greedy-sampler","title":"Greedy sampler","text":"<p>You can also use the greedy sampler. For this you need to initialize the generator with the sampler:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-0.1\")\nsampler = samplers.greedy()\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre> <p>You cannot ask for multiple samples with the greedy sampler since it does not clear what the result should be.</p>"},{"location":"reference/samplers/#beam-search","title":"Beam Search","text":"<p>Outlines also comes with the Beam Search sampling algorithm:</p> <pre><code>from outlines import models, generate, samplers\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-Instruct-v0.2\")\nsampler = samplers.beam_search(beams=5)\n\ngenerator = generate.text(model, sampler)\nanswer = generator(\"What is 2+2?\")\n\nprint(answer)\n# 4\n</code></pre>"},{"location":"reference/text/","title":"Text generation","text":"<p>Outlines provides a unified interface to generate text with many language models, API-based and local:</p> <pre><code>from outlines import models, generate\n\nmodel = models.openai(\"gpt-4\")\ngenerator = generate.text(model)\nanswer = generator(\"What is 2+2?\")\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.text(model)\nanswer = generator(\"What is 2+2?\")\n</code></pre> <p>We generate text in two steps:</p> <ol> <li>Instantiate a generator with the model you want to use</li> <li>Call the generator with the prompt</li> </ol>"},{"location":"reference/text/#limit-the-number-of-tokens-generated","title":"Limit the number of tokens generated","text":"<p>To limit the number of tokens generated you can pass the <code>max_tokens</code> positional argument to the generator:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.text(model)\n\nanswer = generator(\"What is 2+2?\", 5)\nanswer = generator(\"What is 2+2?\", max_tokens=5)\n</code></pre>"},{"location":"reference/text/#stop-when-a-given-string-is-found","title":"Stop when a given string is found","text":"<p>You can also ask the model to stop generating text after a given string has been generated, for instance a period or a line break. You can pass a string or a line of string for the <code>stop_at</code> argument:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.text(model)\n\nanswer = generator(\"What is 2+2?\", stop_at=\".\")\nanswer = generator(\"What is 2+2?\", stop_at=[\".\", \"\\n\"])\n</code></pre>"},{"location":"reference/text/#streaming","title":"Streaming","text":"<p>Outlines allows you to stream the model's response by calling the <code>.stream</code> method of the generator with the prompt:</p> <pre><code>from outlines import models, generate\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.text(model)\n\ntokens = generator.stream(\"What is 2+2?\")\nfor token in tokens:\n    print(token)\n</code></pre>"},{"location":"reference/text/#use-a-different-sampler","title":"Use a different sampler","text":"<p>Outlines uses the multinomial sampler by default. To specify another sampler, for instance the greedy sampler you need to specify it when instantiating the generator:</p> <pre><code>from outlines import models, generate\nfrom outlines.generate.samplers import greedy\n\n\nmodel = models.transformers(\"mistralai/Mistral-7B-v0.1\")\ngenerator = generate.text(model, sampler=greedy)\n\ntokens = generator(\"What is 2+2?\")\n</code></pre>"},{"location":"reference/types/","title":"Type constraints","text":"<p>We can ask completions to be restricted to valid integers or floating-point numbers using the <code>type</code> keyword argument, respectively with the \u201cint\u201d or \u201cfloat\u201d value:</p> <pre><code>import outlines.models as models\n\ncomplete = models.openai(\"gpt-3.5-turbo\")\nanswer = complete(\n    \"When I was 6 my sister was half my age. Now I\u2019m 70 how old is my sister?\",\n    type=\"int\"\n)\n</code></pre>"},{"location":"reference/vllm/","title":"Serve with vLLM","text":"<p>Would rather not self-host?</p> <p>If you want to get started quickly with JSON-structured generaton you can call instead .json, a .txt API that guarantees valid JSON.</p> <p>Outlines can be deployed as an LLM service using the vLLM inference engine and a FastAPI server. vLLM is not installed by default so will need to install Outlines with:</p> <pre><code>pip install outlines[serve]\n</code></pre> <p>You can then start the server with:</p> <pre><code>python -m outlines.serve.serve --model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n</code></pre> <p>This will by default start a server at <code>http://127.0.0.1:8000</code> (check what the console says, though). Without the <code>--model</code> argument set, the OPT-125M model is used. The <code>--model</code> argument allows you to specify any model of your choosing.</p>"},{"location":"reference/vllm/#alternative-method-via-docker","title":"Alternative Method: Via Docker","text":"<p>You can install and run the server with Outlines' official Docker image using the command</p> <pre><code>docker run -p 8000:8000 outlinesdev/outlines --model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n</code></pre>"},{"location":"reference/vllm/#querying-endpoint","title":"Querying Endpoint","text":"<p>You can then query the model in shell by passing a prompt and either</p> <ol> <li>a JSON Schema specification or</li> <li>a Regex pattern</li> </ol> <p>with the <code>schema</code> or <code>regex</code> parameters, respectively, to the <code>/generate</code> endpoint. If both are specified, the schema will be used. If neither is specified, the generated text will be unconstrained.</p> <p>For example, to generate a string that matches the schema <code>{\"type\": \"string\"}</code> (any string):</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"What is the capital of France?\",\n        \"schema\": {\"type\": \"string\", \"maxLength\": 5}\n        }'\n</code></pre> <p>To generate a string that matches the regex <code>(-)?(0|[1-9][0-9]*)(\\.[0-9]+)?([eE][+-][0-9]+)?</code> (a number):</p> <pre><code>curl http://127.0.0.1:8000/generate \\\n    -d '{\n        \"prompt\": \"What is Pi? Give me the first 15 digits: \",\n        \"regex\": \"(-)?(0|[1-9][0-9]*)(\\\\.[0-9]+)?([eE][+-][0-9]+)?\"\n        }'\n</code></pre> <p>Instead of <code>curl</code>, you can also use the requests library from another python program.</p> <p>Please consult the vLLM documentation for details on additional request parameters. You can also read the code in case you need to customize the solution to your needs.</p>"},{"location":"reference/models/llamacpp/","title":"Llama.cpp","text":"<p>Installation</p> <p>You need to install the <code>llama-cpp-python</code> library to be able to use these models in Outlines.</p> <p>Outlines provides an integration with Llama.cpp using the llama-cpp-python library. Llamacpp allows to run quantized models on machines with limited compute.</p> <p>Assuming Phi2's weights are in the current directory:</p> <pre><code>from outlines import models, generate\n\nmodel = models.llamacpp(\"./phi-2.Q4_K_M.gguf\")\n</code></pre>"},{"location":"reference/models/openai/","title":"Generate text with the OpenAI API","text":"<p>Installation</p> <p>You need to install the <code>openai</code> and <code>tiktoken</code> libraries to be able to use the OpenAI API in Outlines.</p> <p>Outlines supports models available via the OpenAI Chat API, e.g. ChatGPT and GPT-4. The following models can be used with Outlines:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-3.5-turbo\")\nmodel = models.openai(\"gpt-4\")\n\nprint(type(model))\n# OpenAI\n</code></pre> <p>It is possible to pass a system message to the model when initializing it:</p> <pre><code>from outlines import models\n\nmodel = models.openai(\"gpt-4\", system_prompt=\"You are a useful assistant\")\n</code></pre> <p>This message will be used for every subsequent use of the model:</p>"},{"location":"reference/models/openai/#monitoring-api-use","title":"Monitoring API use","text":"<p>It is important to be able to track your API usage when working with OpenAI's API. The number of prompt tokens and completion tokens is directly accessible via the model instance:</p> <pre><code>import outlines.models\n\nmodel = models.openai(\"gpt-4\")\n\nprint(model.prompt_tokens)\n# 0\n\nprint(model.completion_tokens)\n# 0\n</code></pre> <p>These numbers are updated every time you call the model.</p>"},{"location":"reference/models/openai/#advanced-usage","title":"Advanced usage","text":"<p>It is possible to specify the values for <code>seed</code>, <code>presence_penalty</code>, <code>frequence_penalty</code>, <code>top_p</code> by passing an instance of <code>OpenAIConfig</code> when initializing the model:</p> <pre><code>from outlines.models.openai import OpenAIConfig\nfrom outlines import models\n\nconfig = OpenAIConfig(\n    presence_penalty=1.,\n    frequence_penalty=1.,\n    top_p=.95,\n    seed=0,\n)\nmodel = models.openai(\"gpt-4\", config=config)\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/roadmap/","title":"Roadmap","text":""}]}